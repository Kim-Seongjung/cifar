{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 3072)\n",
      "32 32\n",
      "32 32\n"
     ]
    }
   ],
   "source": [
    "#conv Neural Network\n",
    "# tensorboard --logdir=/home/ncc/notebook/learn/tensorboard/log\n",
    "\"\"\"\n",
    "created by kim Seong jung\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os \n",
    "\n",
    "file_locate='/home/user01/notebook/cifar_extracted/'\n",
    "sess = tf.InteractiveSession()\n",
    "test_img=np.load(file_locate+'test_img.npy');\n",
    "try:\n",
    "    print np.shape(test_img)\n",
    "    img_row = np.shape(test_img)[1]\n",
    "    img_col = np.shape(test_img)[2]\n",
    "except:\n",
    "    np.shape(test_img)\n",
    "    test_img=np.reshape(test_img , newshape = [np.shape(test_img)[0] , 32, 32 ,3] )\n",
    "    img_row = np.shape(test_img)[1]\n",
    "    img_col = np.shape(test_img)[2]\n",
    "\n",
    "    \n",
    "divide_flag= True\n",
    "batch_size=30\n",
    "print img_row ,img_col\n",
    "n_classes =10\n",
    "in_ch =3\n",
    "out_ch1=200\n",
    "out_ch2=200\n",
    "out_ch3=200\n",
    "out_ch4=200\n",
    "out_ch5=200\n",
    "\n",
    "\n",
    "fully_ch1=1024\n",
    "fully_ch2 =1024\n",
    "fully_ch3 =1024\n",
    "\n",
    "\n",
    "\n",
    "strides_1=[1,2,2,1]\n",
    "strides_2=[1,1,1,1]\n",
    "strides_3=[1,1,1,1]\n",
    "strides_4=[1,1,1,1]\n",
    "strides_5=[1,1,1,1]\n",
    "\n",
    "\n",
    "x= tf.placeholder(\"float\",shape=[None,img_col * img_row * 3],  name = 'x-input')\n",
    "y_=tf.placeholder(\"float\",shape=[None , n_classes] , name = 'y-input')\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "x_image= tf.reshape(x,[-1,img_row,img_col,3])\n",
    "\n",
    "iterate=300000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weight_row =3 ; weight_col=3\n",
    "\n",
    "pooling_row_size1=int(img_row/2)\n",
    "pooling_row_size2=int(pooling_row_size1/2)\n",
    "pooling_row_size3=int(pooling_row_size2/2)\n",
    "pooling_row_size4=int(pooling_row_size3/2)\n",
    "pooling_row_size5=int(pooling_row_size4/2)\n",
    "pooling_col_size1=int(img_col/2)\n",
    "pooling_col_size2=int(pooling_col_size1/2)\n",
    "pooling_col_size3=int(pooling_col_size2/2)\n",
    "pooling_col_size4=int(pooling_col_size3/2)\n",
    "pooling_col_size5=int(pooling_col_size4/2)\n",
    "\n",
    "print img_col , img_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/user01/notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (10000, 3072)\n",
      "Training Data Label (10000, 10)\n",
      "Test Data Label (5000, 10)\n",
      "val Data Label (5000, 10)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:3'):\n",
    "#with tf.device('/gpu:1'):\n",
    "\n",
    "    if divide_flag == False:\n",
    "        train_img=np.load(file_locate+'train_img.npy');\n",
    "        train_lab=np.load(file_locate+'train_lab.npy');\n",
    "        val_img= np.load(file_locate+'val_img.npy');\n",
    "        val_lab = np.load(file_locate+'val_lab.npy');\n",
    "        test_img=np.load(file_locate+'test_img.npy');\n",
    "        test_lab=np.load(file_locate+'test_lab.npy');\n",
    "\n",
    "        print \"Training Data\",np.shape(train_img)\n",
    "        print \"Training Data Label\",np.shape(train_lab)\n",
    "        print \"Test Data Label\",np.shape(test_lab)\n",
    "        print \"val Data Label\" , np.shape(val_img)\n",
    "\n",
    "        n_train= np.shape(train_img)[0]\n",
    "        n_train_lab = np.shape(train_lab)[0]\n",
    "\n",
    "    if divide_flag == True:\n",
    "        train_img=np.load(file_locate+'train_img_1.npy');\n",
    "        train_lab=np.load(file_locate+'train_lab_1.npy');\n",
    "        val_img= np.load(file_locate+'val_img.npy');\n",
    "        val_lab = np.load(file_locate+'val_lab.npy');\n",
    "        test_img=np.load(file_locate+'test_img.npy');\n",
    "        test_lab=np.load(file_locate+'test_lab.npy');\n",
    "\n",
    "        print \"Training Data\",np.shape(train_img)\n",
    "        print \"Training Data Label\",np.shape(train_lab)\n",
    "        print \"Test Data Label\",np.shape(test_lab)\n",
    "        print \"val Data Label\" , np.shape(val_lab)\n",
    "\n",
    "        n_train= np.shape(train_img)[0]\n",
    "        n_train_lab = np.shape(train_lab)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"def weight_variable(name,shape):\n",
    "    #initial = tf.truncated_normal(shape , stddev=0.1)\n",
    "    initial = tf.get_variable(name,shape=shape , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    return tf.Variable(initial)\"\"\"\n",
    "with tf.device('/gpu:0'):\n",
    "    def bias_variable(shape):\n",
    "        initial = tf.constant(0.1 , shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    def next_batch(batch_size , image , label):\n",
    "\n",
    "        a=np.random.randint(np.shape(image)[0] -batch_size)\n",
    "        batch_x = image[a:a+batch_size,:]\n",
    "        batch_y= label[a:a+batch_size,:]\n",
    "        return batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    def conv2d(x,w,strides_):\n",
    "        return tf.nn.conv2d(x,w, strides = strides_, padding='SAME')\n",
    "    def max_pool_2x2(x):\n",
    "        return tf.nn.max_pool(x , ksize=[1,2,2,1] ,strides = [1,2,2,1] , padding = 'SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"layer1\") as scope:\n",
    "    try:\n",
    "        w_conv1 = tf.get_variable(\"W1\",[weight_row,weight_col,3,out_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        w_conv1 = tf.get_variable(\"W1\",[weight_row,weight_col,3,out_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "with tf.variable_scope(\"layer1\") as scope:\n",
    "    try:\n",
    "        b_conv1 = bias_variable([out_ch1])\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        b_conv1 = bias_variable([out_ch1])\n",
    "                \n",
    "            \n",
    "            \n",
    "with tf.variable_scope('layer2') as scope:\n",
    "    try:\n",
    "        w_conv2 = tf.get_variable(\"W2\",[weight_row,weight_col,out_ch1,out_ch2] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        w_conv2 = tf.get_variable(\"W2\",[weight_row,weight_col,out_ch1,out_ch2] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "with tf.variable_scope('layer2') as scope:\n",
    "    try:\n",
    "        b_conv2= bias_variable([out_ch2])\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        b_conv2= bias_variable([out_ch2])\n",
    "                \n",
    "with tf.variable_scope('layer3') as scope:\n",
    "    try:\n",
    "        w_conv3 = tf.get_variable(\"W3\" ,[weight_row,weight_col,out_ch2,out_ch3] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        w_conv3 = tf.get_variable(\"W3\" ,[weight_row,weight_col,out_ch2,out_ch3] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "with tf.variable_scope('layer3') as scope:\n",
    "    try:\n",
    "        b_conv3 = bias_variable([out_ch3])\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        b_conv3 = bias_variable([out_ch3])\n",
    "        \n",
    "with tf.variable_scope('layer4') as scope:\n",
    "    try:\n",
    "        w_conv4 =tf.get_variable(\"W4\" ,[weight_row,weight_col,out_ch3,out_ch4] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        w_conv3 = tf.get_variable(\"W4\" ,[weight_row,weight_col,out_ch2,out_ch3] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "with tf.variable_scope('layer4') as scope:\n",
    "    try:\n",
    "        b_conv4 = bias_variable([out_ch4])\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        b_conv3 = bias_variable([out_ch3])\n",
    "        \n",
    "with tf.variable_scope('layer5') as scope:\n",
    "    try:\n",
    "        w_conv5 = tf.get_variable(\"W5\",[weight_row,weight_col,out_ch4,out_ch5] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        w_conv3 = tf.get_variable(\"W5\" ,[weight_row,weight_col,out_ch2,out_ch3] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "with tf.variable_scope('layer5') as scope:\n",
    "    try:\n",
    "        b_conv5 = bias_variable([out_ch5])\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        b_conv3 = bias_variable([out_ch3])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu:0\", shape=(?, 16, 16, 200), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 8, 8, 200), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"Relu_2:0\", shape=(?, 8, 8, 200), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"Relu_3:0\", shape=(?, 8, 8, 200), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"MaxPool_2:0\", shape=(?, 4, 4, 200), dtype=float32, device=/device:GPU:0)\n"
     ]
    }
   ],
   "source": [
    "#conncect hidden layer \n",
    "with tf.device('/gpu:0'):\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image , w_conv1 ,strides_1)+b_conv1)\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_conv1 , w_conv2 ,strides_2)+b_conv2)\n",
    "    h_conv2 = max_pool_2x2(h_conv2)#pooling\n",
    "    \n",
    "    h_conv3 = tf.nn.relu(conv2d(h_conv2 , w_conv3,strides_3)+b_conv3)\n",
    "    h_conv4 = tf.nn.relu(conv2d(h_conv3 , w_conv4,strides_4)+b_conv4)\n",
    "    h_pool4 = max_pool_2x2(h_conv4) #pooling \n",
    "\n",
    "    h_conv5 = tf.nn.relu(conv2d(h_conv4, w_conv5,strides_5)+b_conv5)\n",
    "    h_conv5= max_pool_2x2(h_conv5) #pooling \n",
    "\n",
    "    print h_conv1\n",
    "    print h_conv2\n",
    "    print h_conv3\n",
    "    print h_conv4\n",
    "    print h_conv5\n",
    "    \n",
    "    \n",
    "    end_conv = h_conv5\n",
    "    #print conv2d(h_pool1 , w_conv2).get_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 3, 200)\n"
     ]
    }
   ],
   "source": [
    "print w_conv1.get_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "end_conv_row=int(h_conv5.get_shape()[1])\n",
    "end_conv_col=int(h_conv5.get_shape()[2])\n",
    "end_conv_ch=int(h_conv5.get_shape()[3])\n",
    "#connect fully connected layer \n",
    "with tf.device('/gpu:0'):\n",
    "    with tf.variable_scope(\"fc1\") as scope:\n",
    "        try:\n",
    "            w_fc1=tf.get_variable(\"fc1_W\",[end_conv_col*end_conv_row*end_conv_ch,fully_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_fc1=tf.get_variable(\"fc1_W\",[end_conv_col*end_conv_row*end_conv_ch,fully_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        try:\n",
    "            b_fc1 = bias_variable([fully_ch1])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_fc1 = bias_variable([fully_ch1])\n",
    "\n",
    "        \n",
    "with tf.device('/gpu:0'): # flat conv layer \n",
    "    end_flat_conv =tf.reshape(end_conv, [-1,end_conv_col*end_conv_row*end_conv_ch])\n",
    "   \n",
    "with tf.device('/gpu:0'): # connect flat layer with fully  connnected layer \n",
    "    h_fc1 = tf.nn.relu(tf.matmul(end_flat_conv , w_fc1)+ b_fc1)\n",
    "    h_fc1 = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    with tf.variable_scope('fc2') as scope:\n",
    "        try:\n",
    "            w_fc2 =tf.get_variable(\"fc2_W\",[fully_ch1 , fully_ch2],initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_fc2 =tf.get_variable(\"fc2_W\",[fully_ch1 , fully_ch2],initializer = tf.contrib.layers.xavier_initializer())\n",
    "        try:\n",
    "            b_fc2 = bias_variable([fully_ch2])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_fc2 = bias_variable([fully_ch2])\n",
    "\n",
    "with tf.device('/gpu:0'):  # join flat layer with fully  connnected layer \n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1 , w_fc2)+b_fc2)\n",
    "    h_fc2= tf.nn.dropout(h_fc2 , keep_prob)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    with tf.variable_scope('fc3') as scope:\n",
    "        try:\n",
    "            w_fc3 =tf.get_variable(\"fc3_W\",[fully_ch2 , fully_ch3],initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_fc3 =tf.get_variable(\"fc3_W\",[fully_ch2 , fully_ch3],initializer = tf.contrib.layers.xavier_initializer())\n",
    "        try:\n",
    "            b_fc3 = bias_variable([fully_ch3])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_fc3 = bias_variable([fully_ch3])\n",
    "\n",
    "with tf.device('/gpu:0'):  # join flat layer with fully  connnected layer \n",
    "    h_fc3 = tf.nn.relu(tf.matmul(h_fc2 , w_fc3)+b_fc3)\n",
    "    h_fc3= tf.nn.dropout(h_fc3 , keep_prob)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "end_fc=h_fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    with tf.variable_scope('fc3') as scope:\n",
    "        try:\n",
    "            w_end =tf.get_variable(\"end_W\",[fully_ch3 , n_classes ],initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_end =tf.get_variable(\"end_W\",[fully_ch3 , n_classes],initializer = tf.contrib.layers.xavier_initializer())\n",
    "        try:\n",
    "            b_end = bias_variable([n_classes])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_end = bias_variable([n_classes])\n",
    "\n",
    "with tf.device('/gpu:0'):  # join flat layer with fully  connnected layer \n",
    "    y_conv = tf.matmul(h_fc3 , w_end)+b_end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is recorded at :9\n"
     ]
    }
   ],
   "source": [
    "#dirname = '/home/ncc/notebook/mammo/result/'\n",
    "\n",
    "dirname='/home/user01/notebook/'\n",
    "    \n",
    "count=0\n",
    "while(True):\n",
    "    if not os.path.isdir(dirname):\n",
    "        os.mkdir(dirname)\n",
    "        break\n",
    "    elif not os.path.isdir(dirname + str(count)):\n",
    "        dirname=dirname+str(count)\n",
    "        os.mkdir(dirname)\n",
    "        break\n",
    "    else:\n",
    "        count+=1\n",
    "print 'it is recorded at :'+str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f=open(dirname+\"/log.txt\",'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch_list(folder_path):\n",
    "    list_files=os.walk(folder_path).next()[2]\n",
    "    print list_files\n",
    "    ret_train_img_list=[]\n",
    "    ret_train_lab_list=[]\n",
    "    for i , ele in enumerate(list_files):\n",
    "\n",
    "        if 'train'  in ele and 'img'in ele:\n",
    "            ret_train_img_list.append(ele)\n",
    "        elif 'train' in ele  and  'lab' in ele:\n",
    "            ret_train_lab_list.append(ele)\n",
    "    return ret_train_img_list ,ret_train_lab_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_lab.npy', 'train_lab_2.npy', 'val_lab.npy', 'train_img_3.npy', 'train_img_1.npy', 'train_lab_1.npy', 'test_img.npy', 'val_img.npy', 'train_lab_3.npy', 'train_img_2.npy', 'train_lab_5.npy', 'train_img_5.npy', 'train_lab_4.npy', 'train_img_4.npy']\n"
     ]
    }
   ],
   "source": [
    "train_images , train_labels  = get_batch_list(file_locate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_img_1.npy', 'train_img_2.npy', 'train_img_3.npy', 'train_img_4.npy', 'train_img_5.npy']\n",
      "['train_lab_1.npy', 'train_lab_2.npy', 'train_lab_3.npy', 'train_lab_4.npy', 'train_lab_5.npy']\n"
     ]
    }
   ],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "\n",
    "train_images.sort(key=natural_keys)\n",
    "train_labels.sort(key = natural_keys)\n",
    "print(train_images)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 , training  accuracy 0.133333\n",
      "step 0 , loss : 19.3078\n",
      "step 0 , validation  accuracy 0.0944\n",
      "step 0 , validation loss : 2698.01\n",
      "step 100 , training  accuracy 0.233333\n",
      "step 100 , loss : 2.31365\n",
      "step 100 , validation  accuracy 0.1812\n",
      "step 100 , validation loss : 9.39859\n",
      "step 200 , training  accuracy 0.233333\n",
      "step 200 , loss : 2.26859\n",
      "step 200 , validation  accuracy 0.2364\n",
      "step 200 , validation loss : 7.48005\n",
      "step 300 , training  accuracy 0.3\n",
      "step 300 , loss : 2.27911\n",
      "step 300 , validation  accuracy 0.2678\n",
      "step 300 , validation loss : 7.92366\n",
      "step 400 , training  accuracy 0.266667\n",
      "step 400 , loss : 2.27338\n",
      "step 400 , validation  accuracy 0.2858\n",
      "step 400 , validation loss : 8.65942\n",
      "step 500 , training  accuracy 0.4\n",
      "step 500 , loss : 2.21854\n",
      "step 500 , validation  accuracy 0.2868\n",
      "step 500 , validation loss : 8.73612\n",
      "step 600 , training  accuracy 0.166667\n",
      "step 600 , loss : 2.28446\n",
      "step 600 , validation  accuracy 0.2728\n",
      "step 600 , validation loss : 9.57153\n",
      "step 700 , training  accuracy 0.233333\n",
      "step 700 , loss : 2.26508\n",
      "step 700 , validation  accuracy 0.303\n",
      "step 700 , validation loss : 10.7051\n",
      "step 800 , training  accuracy 0.266667\n",
      "step 800 , loss : 2.24748\n",
      "step 800 , validation  accuracy 0.3374\n",
      "step 800 , validation loss : 9.79928\n",
      "step 900 , training  accuracy 0.266667\n",
      "step 900 , loss : 2.27043\n",
      "step 900 , validation  accuracy 0.3176\n",
      "step 900 , validation loss : 11.215\n",
      "step 1000 , training  accuracy 0.333333\n",
      "step 1000 , loss : 2.21577\n",
      "step 1000 , validation  accuracy 0.3416\n",
      "step 1000 , validation loss : 13.6999\n",
      "step 1100 , training  accuracy 0.433333\n",
      "step 1100 , loss : 2.21592\n",
      "step 1100 , validation  accuracy 0.3456\n",
      "step 1100 , validation loss : 12.2625\n",
      "step 1200 , training  accuracy 0.366667\n",
      "step 1200 , loss : 2.22623\n",
      "step 1200 , validation  accuracy 0.3476\n",
      "step 1200 , validation loss : 13.4359\n",
      "step 1300 , training  accuracy 0.366667\n",
      "step 1300 , loss : 2.20993\n",
      "step 1300 , validation  accuracy 0.3562\n",
      "step 1300 , validation loss : 11.0684\n",
      "step 1400 , training  accuracy 0.333333\n",
      "step 1400 , loss : 2.22779\n",
      "step 1400 , validation  accuracy 0.3338\n",
      "step 1400 , validation loss : 11.3871\n",
      "step 1500 , training  accuracy 0.4\n",
      "step 1500 , loss : 2.18867\n",
      "step 1500 , validation  accuracy 0.3718\n",
      "step 1500 , validation loss : 13.4915\n",
      "step 1600 , training  accuracy 0.433333\n",
      "step 1600 , loss : 2.16382\n",
      "step 1600 , validation  accuracy 0.3774\n",
      "step 1600 , validation loss : 14.0645\n",
      "step 1700 , training  accuracy 0.6\n",
      "step 1700 , loss : 2.15752\n",
      "step 1700 , validation  accuracy 0.3794\n",
      "step 1700 , validation loss : 13.8242\n",
      "step 1800 , training  accuracy 0.466667\n",
      "step 1800 , loss : 2.18163\n",
      "step 1800 , validation  accuracy 0.4028\n",
      "step 1800 , validation loss : 12.3823\n",
      "step 1900 , training  accuracy 0.5\n",
      "step 1900 , loss : 2.17423\n",
      "step 1900 , validation  accuracy 0.3682\n",
      "step 1900 , validation loss : 13.8091\n",
      "step 2000 , training  accuracy 0.5\n",
      "step 2000 , loss : 2.1553\n",
      "step 2000 , validation  accuracy 0.404\n",
      "step 2000 , validation loss : 13.2281\n",
      "step 2100 , training  accuracy 0.4\n",
      "step 2100 , loss : 2.20331\n",
      "step 2100 , validation  accuracy 0.397\n",
      "step 2100 , validation loss : 15.1206\n",
      "step 2200 , training  accuracy 0.533333\n",
      "step 2200 , loss : 2.13606\n",
      "step 2200 , validation  accuracy 0.4112\n",
      "step 2200 , validation loss : 16.3086\n",
      "step 2300 , training  accuracy 0.566667\n",
      "step 2300 , loss : 2.17335\n",
      "step 2300 , validation  accuracy 0.3986\n",
      "step 2300 , validation loss : 12.5218\n",
      "step 2400 , training  accuracy 0.466667\n",
      "step 2400 , loss : 2.1612\n",
      "step 2400 , validation  accuracy 0.4218\n",
      "step 2400 , validation loss : 15.2877\n",
      "step 2500 , training  accuracy 0.566667\n",
      "step 2500 , loss : 2.14192\n",
      "step 2500 , validation  accuracy 0.4062\n",
      "step 2500 , validation loss : 14.7474\n",
      "step 2600 , training  accuracy 0.5\n",
      "step 2600 , loss : 2.14266\n",
      "step 2600 , validation  accuracy 0.4288\n",
      "step 2600 , validation loss : 16.7074\n",
      "step 2700 , training  accuracy 0.433333\n",
      "step 2700 , loss : 2.21201\n",
      "step 2700 , validation  accuracy 0.3888\n",
      "step 2700 , validation loss : 15.368\n",
      "step 2800 , training  accuracy 0.533333\n",
      "step 2800 , loss : 2.1398\n",
      "step 2800 , validation  accuracy 0.4254\n",
      "step 2800 , validation loss : 15.1344\n",
      "step 2900 , training  accuracy 0.433333\n",
      "step 2900 , loss : 2.17295\n",
      "step 2900 , validation  accuracy 0.442\n",
      "step 2900 , validation loss : 14.0895\n",
      "step 3000 , training  accuracy 0.533333\n",
      "step 3000 , loss : 2.13269\n",
      "step 3000 , validation  accuracy 0.4262\n",
      "step 3000 , validation loss : 18.333\n",
      "step 3100 , training  accuracy 0.466667\n",
      "step 3100 , loss : 2.11409\n",
      "step 3100 , validation  accuracy 0.4374\n",
      "step 3100 , validation loss : 16.2477\n",
      "step 3200 , training  accuracy 0.5\n",
      "step 3200 , loss : 2.14649\n",
      "step 3200 , validation  accuracy 0.437\n",
      "step 3200 , validation loss : 17.994\n",
      "step 3300 , training  accuracy 0.566667\n",
      "step 3300 , loss : 2.15066\n",
      "step 3300 , validation  accuracy 0.4198\n",
      "step 3300 , validation loss : 16.919\n",
      "step 3400 , training  accuracy 0.366667\n",
      "step 3400 , loss : 2.14201\n",
      "step 3400 , validation  accuracy 0.3776\n",
      "step 3400 , validation loss : 16.1768\n",
      "step 3500 , training  accuracy 0.366667\n",
      "step 3500 , loss : 2.18342\n",
      "step 3500 , validation  accuracy 0.4466\n",
      "step 3500 , validation loss : 16.3285\n",
      "step 3600 , training  accuracy 0.6\n",
      "step 3600 , loss : 2.09987\n",
      "step 3600 , validation  accuracy 0.4344\n",
      "step 3600 , validation loss : 18.1513\n",
      "step 3700 , training  accuracy 0.3\n",
      "step 3700 , loss : 2.21437\n",
      "step 3700 , validation  accuracy 0.4608\n",
      "step 3700 , validation loss : 19.1131\n",
      "step 3800 , training  accuracy 0.566667\n",
      "step 3800 , loss : 2.0903\n",
      "step 3800 , validation  accuracy 0.4524\n",
      "step 3800 , validation loss : 21.6052\n",
      "step 3900 , training  accuracy 0.433333\n",
      "step 3900 , loss : 2.17719\n",
      "step 3900 , validation  accuracy 0.4492\n",
      "step 3900 , validation loss : 19.1405\n",
      "step 4000 , training  accuracy 0.5\n",
      "step 4000 , loss : 2.13098\n",
      "step 4000 , validation  accuracy 0.4576\n",
      "step 4000 , validation loss : 20.9163\n",
      "step 4100 , training  accuracy 0.366667\n",
      "step 4100 , loss : 2.20865\n",
      "step 4100 , validation  accuracy 0.483\n",
      "step 4100 , validation loss : 20.1153\n",
      "step 4200 , training  accuracy 0.466667\n",
      "step 4200 , loss : 2.15597\n",
      "step 4200 , validation  accuracy 0.4798\n",
      "step 4200 , validation loss : 18.9952\n",
      "step 4300 , training  accuracy 0.666667\n",
      "step 4300 , loss : 2.09334\n",
      "step 4300 , validation  accuracy 0.4682\n",
      "step 4300 , validation loss : 19.542\n",
      "step 4400 , training  accuracy 0.466667\n",
      "step 4400 , loss : 2.1468\n",
      "step 4400 , validation  accuracy 0.455\n",
      "step 4400 , validation loss : 21.5429\n",
      "step 4500 , training  accuracy 0.566667\n",
      "step 4500 , loss : 2.0401\n",
      "step 4500 , validation  accuracy 0.4778\n",
      "step 4500 , validation loss : 23.5829\n",
      "step 4600 , training  accuracy 0.466667\n",
      "step 4600 , loss : 2.13004\n",
      "step 4600 , validation  accuracy 0.4776\n",
      "step 4600 , validation loss : 21.8471\n",
      "step 4700 , training  accuracy 0.6\n",
      "step 4700 , loss : 2.03062\n",
      "step 4700 , validation  accuracy 0.493\n",
      "step 4700 , validation loss : 22.7446\n",
      "step 4800 , training  accuracy 0.5\n",
      "step 4800 , loss : 2.11891\n",
      "step 4800 , validation  accuracy 0.4576\n",
      "step 4800 , validation loss : 20.4418\n",
      "step 4900 , training  accuracy 0.566667\n",
      "step 4900 , loss : 2.08793\n",
      "step 4900 , validation  accuracy 0.4842\n",
      "step 4900 , validation loss : 21.8358\n",
      "step 5000 , training  accuracy 0.533333\n",
      "step 5000 , loss : 2.12464\n",
      "step 5000 , validation  accuracy 0.4798\n",
      "step 5000 , validation loss : 22.3561\n",
      "step 5100 , training  accuracy 0.533333\n",
      "step 5100 , loss : 2.09214\n",
      "step 5100 , validation  accuracy 0.4844\n",
      "step 5100 , validation loss : 23.2467\n",
      "step 5200 , training  accuracy 0.466667\n",
      "step 5200 , loss : 2.1145\n",
      "step 5200 , validation  accuracy 0.4804\n",
      "step 5200 , validation loss : 23.3049\n",
      "step 5300 , training  accuracy 0.733333\n",
      "step 5300 , loss : 2.0545\n",
      "step 5300 , validation  accuracy 0.4714\n",
      "step 5300 , validation loss : 21.9851\n",
      "step 5400 , training  accuracy 0.4\n",
      "step 5400 , loss : 2.17877\n",
      "step 5400 , validation  accuracy 0.5058\n",
      "step 5400 , validation loss : 23.6898\n",
      "step 5500 , training  accuracy 0.6\n",
      "step 5500 , loss : 2.06824\n",
      "step 5500 , validation  accuracy 0.5064\n",
      "step 5500 , validation loss : 20.6661\n",
      "step 5600 , training  accuracy 0.466667\n",
      "step 5600 , loss : 2.15463\n",
      "step 5600 , validation  accuracy 0.5002\n",
      "step 5600 , validation loss : 23.8047\n",
      "step 5700 , training  accuracy 0.433333\n",
      "step 5700 , loss : 2.15052\n",
      "step 5700 , validation  accuracy 0.49\n",
      "step 5700 , validation loss : 23.305\n",
      "step 5800 , training  accuracy 0.633333\n",
      "step 5800 , loss : 2.0802\n",
      "step 5800 , validation  accuracy 0.4576\n",
      "step 5800 , validation loss : 24.91\n",
      "step 5900 , training  accuracy 0.6\n",
      "step 5900 , loss : 2.07861\n",
      "step 5900 , validation  accuracy 0.518\n",
      "step 5900 , validation loss : 24.1559\n",
      "step 6000 , training  accuracy 0.633333\n",
      "step 6000 , loss : 2.09048\n",
      "step 6000 , validation  accuracy 0.5176\n",
      "step 6000 , validation loss : 24.9847\n",
      "step 6100 , training  accuracy 0.666667\n",
      "step 6100 , loss : 2.04514\n",
      "step 6100 , validation  accuracy 0.5216\n",
      "step 6100 , validation loss : 24.5626\n",
      "step 6200 , training  accuracy 0.6\n",
      "step 6200 , loss : 2.06481\n",
      "step 6200 , validation  accuracy 0.5034\n",
      "step 6200 , validation loss : 23.0854\n",
      "step 6300 , training  accuracy 0.5\n",
      "step 6300 , loss : 2.13089\n",
      "step 6300 , validation  accuracy 0.5084\n",
      "step 6300 , validation loss : 24.7467\n",
      "step 6400 , training  accuracy 0.633333\n",
      "step 6400 , loss : 2.05076\n",
      "step 6400 , validation  accuracy 0.5068\n",
      "step 6400 , validation loss : 25.8229\n",
      "step 6500 , training  accuracy 0.666667\n",
      "step 6500 , loss : 2.01083\n",
      "step 6500 , validation  accuracy 0.5274\n",
      "step 6500 , validation loss : 26.6371\n",
      "step 6600 , training  accuracy 0.633333\n",
      "step 6600 , loss : 2.02312\n",
      "step 6600 , validation  accuracy 0.5108\n",
      "step 6600 , validation loss : 25.756\n",
      "step 6700 , training  accuracy 0.6\n",
      "step 6700 , loss : 2.03071\n",
      "step 6700 , validation  accuracy 0.5268\n",
      "step 6700 , validation loss : 24.3537\n",
      "step 6800 , training  accuracy 0.633333\n",
      "step 6800 , loss : 2.03988\n",
      "step 6800 , validation  accuracy 0.5236\n",
      "step 6800 , validation loss : 26.8849\n",
      "step 6900 , training  accuracy 0.6\n",
      "step 6900 , loss : 2.06912\n",
      "step 6900 , validation  accuracy 0.5218\n",
      "step 6900 , validation loss : 27.8593\n",
      "step 7000 , training  accuracy 0.733333\n",
      "step 7000 , loss : 1.99148\n",
      "step 7000 , validation  accuracy 0.5344\n",
      "step 7000 , validation loss : 26.9846\n",
      "step 7100 , training  accuracy 0.566667\n",
      "step 7100 , loss : 2.04506\n",
      "step 7100 , validation  accuracy 0.533\n",
      "step 7100 , validation loss : 28.8093\n",
      "step 7200 , training  accuracy 0.6\n",
      "step 7200 , loss : 2.06222\n",
      "step 7200 , validation  accuracy 0.5438\n",
      "step 7200 , validation loss : 26.9485\n",
      "step 7300 , training  accuracy 0.433333\n",
      "step 7300 , loss : 2.16503\n",
      "step 7300 , validation  accuracy 0.5278\n",
      "step 7300 , validation loss : 26.4384\n",
      "step 7400 , training  accuracy 0.533333\n",
      "step 7400 , loss : 2.09444\n",
      "step 7400 , validation  accuracy 0.519\n",
      "step 7400 , validation loss : 27.8254\n",
      "step 7500 , training  accuracy 0.433333\n",
      "step 7500 , loss : 2.19408\n",
      "step 7500 , validation  accuracy 0.523\n",
      "step 7500 , validation loss : 31.5893\n",
      "step 7600 , training  accuracy 0.466667\n",
      "step 7600 , loss : 2.1306\n",
      "step 7600 , validation  accuracy 0.5202\n",
      "step 7600 , validation loss : 30.0834\n",
      "step 7700 , training  accuracy 0.633333\n",
      "step 7700 , loss : 2.01808\n",
      "step 7700 , validation  accuracy 0.5282\n",
      "step 7700 , validation loss : 29.5222\n",
      "step 7800 , training  accuracy 0.666667\n",
      "step 7800 , loss : 2.03183\n",
      "step 7800 , validation  accuracy 0.5266\n",
      "step 7800 , validation loss : 30.4597\n",
      "step 7900 , training  accuracy 0.5\n",
      "step 7900 , loss : 2.06853\n",
      "step 7900 , validation  accuracy 0.5452\n",
      "step 7900 , validation loss : 28.7047\n",
      "step 8000 , training  accuracy 0.433333\n",
      "step 8000 , loss : 2.17308\n",
      "step 8000 , validation  accuracy 0.5304\n",
      "step 8000 , validation loss : 28.3695\n",
      "step 8100 , training  accuracy 0.633333\n",
      "step 8100 , loss : 1.99896\n",
      "step 8100 , validation  accuracy 0.5474\n",
      "step 8100 , validation loss : 29.6063\n",
      "step 8200 , training  accuracy 0.5\n",
      "step 8200 , loss : 2.08783\n",
      "step 8200 , validation  accuracy 0.5526\n",
      "step 8200 , validation loss : 31.8519\n",
      "step 8300 , training  accuracy 0.666667\n",
      "step 8300 , loss : 2.01478\n",
      "step 8300 , validation  accuracy 0.547\n",
      "step 8300 , validation loss : 33.5728\n",
      "step 8400 , training  accuracy 0.5\n",
      "step 8400 , loss : 2.12925\n",
      "step 8400 , validation  accuracy 0.5506\n",
      "step 8400 , validation loss : 31.4684\n",
      "step 8500 , training  accuracy 0.733333\n",
      "step 8500 , loss : 1.94951\n",
      "step 8500 , validation  accuracy 0.5576\n",
      "step 8500 , validation loss : 32.9654\n",
      "step 8600 , training  accuracy 0.733333\n",
      "step 8600 , loss : 1.93962\n",
      "step 8600 , validation  accuracy 0.5602\n",
      "step 8600 , validation loss : 31.6256\n",
      "step 8700 , training  accuracy 0.766667\n",
      "step 8700 , loss : 1.92457\n",
      "step 8700 , validation  accuracy 0.548\n",
      "step 8700 , validation loss : 34.4709\n",
      "step 8800 , training  accuracy 0.633333\n",
      "step 8800 , loss : 2.0306\n",
      "step 8800 , validation  accuracy 0.5524\n",
      "step 8800 , validation loss : 33.6975\n",
      "step 8900 , training  accuracy 0.8\n",
      "step 8900 , loss : 1.88234\n",
      "step 8900 , validation  accuracy 0.5632\n",
      "step 8900 , validation loss : 32.3968\n",
      "step 9000 , training  accuracy 0.6\n",
      "step 9000 , loss : 2.07473\n",
      "step 9000 , validation  accuracy 0.5546\n",
      "step 9000 , validation loss : 34.0785\n",
      "step 9100 , training  accuracy 0.6\n",
      "step 9100 , loss : 2.05626\n",
      "step 9100 , validation  accuracy 0.5594\n",
      "step 9100 , validation loss : 33.4711\n",
      "step 9200 , training  accuracy 0.766667\n",
      "step 9200 , loss : 1.95894\n",
      "step 9200 , validation  accuracy 0.5676\n",
      "step 9200 , validation loss : 32.0291\n",
      "step 9300 , training  accuracy 0.8\n",
      "step 9300 , loss : 1.90424\n",
      "step 9300 , validation  accuracy 0.5414\n",
      "step 9300 , validation loss : 33.6637\n",
      "step 9400 , training  accuracy 0.666667\n",
      "step 9400 , loss : 2.02461\n",
      "step 9400 , validation  accuracy 0.565\n",
      "step 9400 , validation loss : 32.1107\n",
      "step 9500 , training  accuracy 0.6\n",
      "step 9500 , loss : 2.02646\n",
      "step 9500 , validation  accuracy 0.5672\n",
      "step 9500 , validation loss : 35.1567\n",
      "step 9600 , training  accuracy 0.6\n",
      "step 9600 , loss : 2.09671\n",
      "step 9600 , validation  accuracy 0.5652\n",
      "step 9600 , validation loss : 31.2747\n",
      "step 9700 , training  accuracy 0.566667\n",
      "step 9700 , loss : 2.06006\n",
      "step 9700 , validation  accuracy 0.5584\n",
      "step 9700 , validation loss : 34.3645\n",
      "step 9800 , training  accuracy 0.666667\n",
      "step 9800 , loss : 2.0002\n",
      "step 9800 , validation  accuracy 0.5558\n",
      "step 9800 , validation loss : 35.3498\n",
      "step 9900 , training  accuracy 0.7\n",
      "step 9900 , loss : 1.99394\n",
      "step 9900 , validation  accuracy 0.5604\n",
      "step 9900 , validation loss : 33.4735\n",
      "step 10000 , training  accuracy 0.733333\n",
      "step 10000 , loss : 2.01476\n",
      "step 10000 , validation  accuracy 0.5668\n",
      "step 10000 , validation loss : 35.12\n",
      "step 10100 , training  accuracy 0.7\n",
      "step 10100 , loss : 1.98515\n",
      "step 10100 , validation  accuracy 0.5744\n",
      "step 10100 , validation loss : 31.74\n",
      "step 10200 , training  accuracy 0.733333\n",
      "step 10200 , loss : 2.00969\n",
      "step 10200 , validation  accuracy 0.5788\n",
      "step 10200 , validation loss : 35.8607\n",
      "step 10300 , training  accuracy 0.666667\n",
      "step 10300 , loss : 2.0117\n",
      "step 10300 , validation  accuracy 0.5564\n",
      "step 10300 , validation loss : 33.0706\n",
      "step 10400 , training  accuracy 0.766667\n",
      "step 10400 , loss : 1.93541\n",
      "step 10400 , validation  accuracy 0.5704\n",
      "step 10400 , validation loss : 34.5608\n",
      "step 10500 , training  accuracy 0.566667\n",
      "step 10500 , loss : 2.03976\n",
      "step 10500 , validation  accuracy 0.5626\n",
      "step 10500 , validation loss : 35.6567\n",
      "step 10600 , training  accuracy 0.733333\n",
      "step 10600 , loss : 2.02642\n",
      "step 10600 , validation  accuracy 0.573\n",
      "step 10600 , validation loss : 34.3151\n",
      "step 10700 , training  accuracy 0.766667\n",
      "step 10700 , loss : 1.9486\n",
      "step 10700 , validation  accuracy 0.58\n",
      "step 10700 , validation loss : 36.5516\n",
      "step 10800 , training  accuracy 0.566667\n",
      "step 10800 , loss : 2.06044\n",
      "step 10800 , validation  accuracy 0.5832\n",
      "step 10800 , validation loss : 37.8828\n",
      "step 10900 , training  accuracy 0.533333\n",
      "step 10900 , loss : 2.07128\n",
      "step 10900 , validation  accuracy 0.5842\n",
      "step 10900 , validation loss : 38.1168\n",
      "step 11000 , training  accuracy 0.6\n",
      "step 11000 , loss : 2.0293\n",
      "step 11000 , validation  accuracy 0.573\n",
      "step 11000 , validation loss : 34.6329\n",
      "step 11100 , training  accuracy 0.7\n",
      "step 11100 , loss : 1.96978\n",
      "step 11100 , validation  accuracy 0.5708\n",
      "step 11100 , validation loss : 34.1734\n",
      "step 11200 , training  accuracy 0.766667\n",
      "step 11200 , loss : 1.9102\n",
      "step 11200 , validation  accuracy 0.584\n",
      "step 11200 , validation loss : 37.1921\n",
      "step 11300 , training  accuracy 0.666667\n",
      "step 11300 , loss : 1.97447\n",
      "step 11300 , validation  accuracy 0.5824\n",
      "step 11300 , validation loss : 36.6211\n",
      "step 11400 , training  accuracy 0.6\n",
      "step 11400 , loss : 2.04198\n",
      "step 11400 , validation  accuracy 0.5736\n",
      "step 11400 , validation loss : 38.8095\n",
      "step 11500 , training  accuracy 0.8\n",
      "step 11500 , loss : 1.8897\n",
      "step 11500 , validation  accuracy 0.5908\n",
      "step 11500 , validation loss : 38.4828\n",
      "step 11600 , training  accuracy 0.666667\n",
      "step 11600 , loss : 1.94753\n",
      "step 11600 , validation  accuracy 0.5718\n",
      "step 11600 , validation loss : 35.3511\n",
      "step 11700 , training  accuracy 0.6\n",
      "step 11700 , loss : 2.09423\n",
      "step 11700 , validation  accuracy 0.5728\n",
      "step 11700 , validation loss : 36.1141\n",
      "step 11800 , training  accuracy 0.8\n",
      "step 11800 , loss : 1.92834\n",
      "step 11800 , validation  accuracy 0.5804\n",
      "step 11800 , validation loss : 35.6489\n",
      "step 11900 , training  accuracy 0.666667\n",
      "step 11900 , loss : 2.01466\n",
      "step 11900 , validation  accuracy 0.579\n",
      "step 11900 , validation loss : 36.0334\n",
      "step 12000 , training  accuracy 0.8\n",
      "step 12000 , loss : 1.9371\n",
      "step 12000 , validation  accuracy 0.5812\n",
      "step 12000 , validation loss : 38.504\n",
      "step 12100 , training  accuracy 0.6\n",
      "step 12100 , loss : 2.07361\n",
      "step 12100 , validation  accuracy 0.5604\n",
      "step 12100 , validation loss : 36.9053\n",
      "step 12200 , training  accuracy 0.733333\n",
      "step 12200 , loss : 1.94497\n",
      "step 12200 , validation  accuracy 0.5806\n",
      "step 12200 , validation loss : 37.6377\n",
      "step 12300 , training  accuracy 0.866667\n",
      "step 12300 , loss : 1.88289\n",
      "step 12300 , validation  accuracy 0.5766\n",
      "step 12300 , validation loss : 38.3935\n",
      "step 12400 , training  accuracy 0.733333\n",
      "step 12400 , loss : 1.92934\n",
      "step 12400 , validation  accuracy 0.5772\n",
      "step 12400 , validation loss : 36.0348\n",
      "step 12500 , training  accuracy 0.766667\n",
      "step 12500 , loss : 1.92253\n",
      "step 12500 , validation  accuracy 0.5934\n",
      "step 12500 , validation loss : 40.0019\n",
      "step 12600 , training  accuracy 0.666667\n",
      "step 12600 , loss : 2.00337\n",
      "step 12600 , validation  accuracy 0.5704\n",
      "step 12600 , validation loss : 36.1136\n",
      "step 12700 , training  accuracy 0.833333\n",
      "step 12700 , loss : 1.90376\n",
      "step 12700 , validation  accuracy 0.5834\n",
      "step 12700 , validation loss : 36.7982\n",
      "step 12800 , training  accuracy 0.733333\n",
      "step 12800 , loss : 1.94152\n",
      "step 12800 , validation  accuracy 0.5962\n",
      "step 12800 , validation loss : 40.5181\n",
      "step 12900 , training  accuracy 0.7\n",
      "step 12900 , loss : 1.95667\n",
      "step 12900 , validation  accuracy 0.584\n",
      "step 12900 , validation loss : 40.3232\n",
      "step 13000 , training  accuracy 0.666667\n",
      "step 13000 , loss : 2.0132\n",
      "step 13000 , validation  accuracy 0.5708\n",
      "step 13000 , validation loss : 39.9773\n",
      "step 13100 , training  accuracy 0.666667\n",
      "step 13100 , loss : 1.95259\n",
      "step 13100 , validation  accuracy 0.5906\n",
      "step 13100 , validation loss : 38.1596\n",
      "step 13200 , training  accuracy 0.8\n",
      "step 13200 , loss : 1.932\n",
      "step 13200 , validation  accuracy 0.5964\n",
      "step 13200 , validation loss : 39.0641\n",
      "step 13300 , training  accuracy 0.766667\n",
      "step 13300 , loss : 1.92495\n",
      "step 13300 , validation  accuracy 0.572\n",
      "step 13300 , validation loss : 38.6958\n",
      "step 13400 , training  accuracy 0.766667\n",
      "step 13400 , loss : 1.91496\n",
      "step 13400 , validation  accuracy 0.5964\n",
      "step 13400 , validation loss : 38.7802\n",
      "step 13500 , training  accuracy 0.9\n",
      "step 13500 , loss : 1.83684\n",
      "step 13500 , validation  accuracy 0.5846\n",
      "step 13500 , validation loss : 38.8626\n",
      "step 13600 , training  accuracy 0.8\n",
      "step 13600 , loss : 1.89954\n",
      "step 13600 , validation  accuracy 0.6\n",
      "step 13600 , validation loss : 38.8298\n",
      "step 13700 , training  accuracy 0.7\n",
      "step 13700 , loss : 1.94696\n",
      "step 13700 , validation  accuracy 0.5766\n",
      "step 13700 , validation loss : 42.2408\n",
      "step 13800 , training  accuracy 0.933333\n",
      "step 13800 , loss : 1.79192\n",
      "step 13800 , validation  accuracy 0.5996\n",
      "step 13800 , validation loss : 39.1683\n",
      "step 13900 , training  accuracy 0.8\n",
      "step 13900 , loss : 1.90068\n",
      "step 13900 , validation  accuracy 0.6016\n",
      "step 13900 , validation loss : 41.1079\n",
      "step 14000 , training  accuracy 0.833333\n",
      "step 14000 , loss : 1.88079\n",
      "step 14000 , validation  accuracy 0.59\n",
      "step 14000 , validation loss : 40.1406\n",
      "step 14100 , training  accuracy 0.733333\n",
      "step 14100 , loss : 1.93459\n",
      "step 14100 , validation  accuracy 0.5856\n",
      "step 14100 , validation loss : 40.7318\n",
      "step 14200 , training  accuracy 0.833333\n",
      "step 14200 , loss : 1.89844\n",
      "step 14200 , validation  accuracy 0.5976\n",
      "step 14200 , validation loss : 40.1613\n",
      "step 14300 , training  accuracy 0.7\n",
      "step 14300 , loss : 1.94285\n",
      "step 14300 , validation  accuracy 0.5856\n",
      "step 14300 , validation loss : 41.5181\n",
      "step 14400 , training  accuracy 0.833333\n",
      "step 14400 , loss : 1.88494\n",
      "step 14400 , validation  accuracy 0.5962\n",
      "step 14400 , validation loss : 41.3063\n",
      "step 14500 , training  accuracy 0.8\n",
      "step 14500 , loss : 1.88805\n",
      "step 14500 , validation  accuracy 0.5914\n",
      "step 14500 , validation loss : 41.3155\n",
      "step 14600 , training  accuracy 0.866667\n",
      "step 14600 , loss : 1.86276\n",
      "step 14600 , validation  accuracy 0.59\n",
      "step 14600 , validation loss : 41.7396\n",
      "step 14700 , training  accuracy 0.733333\n",
      "step 14700 , loss : 1.92345\n",
      "step 14700 , validation  accuracy 0.599\n",
      "step 14700 , validation loss : 40.3461\n",
      "step 14800 , training  accuracy 0.633333\n",
      "step 14800 , loss : 1.98113\n",
      "step 14800 , validation  accuracy 0.5954\n",
      "step 14800 , validation loss : 40.8566\n",
      "step 14900 , training  accuracy 0.633333\n",
      "step 14900 , loss : 2.00948\n",
      "step 14900 , validation  accuracy 0.5864\n",
      "step 14900 , validation loss : 43.0992\n",
      "step 15000 , training  accuracy 0.633333\n",
      "step 15000 , loss : 2.0911\n",
      "step 15000 , validation  accuracy 0.5996\n",
      "step 15000 , validation loss : 42.6132\n",
      "step 15100 , training  accuracy 0.733333\n",
      "step 15100 , loss : 1.91331\n",
      "step 15100 , validation  accuracy 0.6066\n",
      "step 15100 , validation loss : 41.694\n",
      "step 15200 , training  accuracy 0.7\n",
      "step 15200 , loss : 1.94819\n",
      "step 15200 , validation  accuracy 0.5798\n",
      "step 15200 , validation loss : 41.0568\n",
      "step 15300 , training  accuracy 0.8\n",
      "step 15300 , loss : 1.90268\n",
      "step 15300 , validation  accuracy 0.5844\n",
      "step 15300 , validation loss : 43.9869\n",
      "step 15400 , training  accuracy 0.733333\n",
      "step 15400 , loss : 1.8852\n",
      "step 15400 , validation  accuracy 0.6012\n",
      "step 15400 , validation loss : 41.6015\n",
      "step 15500 , training  accuracy 0.666667\n",
      "step 15500 , loss : 2.01207\n",
      "step 15500 , validation  accuracy 0.6028\n",
      "step 15500 , validation loss : 41.977\n",
      "step 15600 , training  accuracy 0.633333\n",
      "step 15600 , loss : 2.0163\n",
      "step 15600 , validation  accuracy 0.5884\n",
      "step 15600 , validation loss : 38.9165\n",
      "step 15700 , training  accuracy 0.8\n",
      "step 15700 , loss : 1.85647\n",
      "step 15700 , validation  accuracy 0.5946\n",
      "step 15700 , validation loss : 43.7967\n",
      "step 15800 , training  accuracy 0.966667\n",
      "step 15800 , loss : 1.75415\n",
      "step 15800 , validation  accuracy 0.6038\n",
      "step 15800 , validation loss : 43.3657\n",
      "step 15900 , training  accuracy 0.8\n",
      "step 15900 , loss : 1.88334\n",
      "step 15900 , validation  accuracy 0.6002\n",
      "step 15900 , validation loss : 44.2232\n",
      "step 16000 , training  accuracy 0.9\n",
      "step 16000 , loss : 1.80233\n",
      "step 16000 , validation  accuracy 0.5966\n",
      "step 16000 , validation loss : 44.1048\n",
      "step 16100 , training  accuracy 0.733333\n",
      "step 16100 , loss : 1.94283\n",
      "step 16100 , validation  accuracy 0.5952\n",
      "step 16100 , validation loss : 40.6738\n",
      "step 16200 , training  accuracy 0.8\n",
      "step 16200 , loss : 1.96736\n",
      "step 16200 , validation  accuracy 0.5856\n",
      "step 16200 , validation loss : 44.3328\n",
      "step 16300 , training  accuracy 0.9\n",
      "step 16300 , loss : 1.8477\n",
      "step 16300 , validation  accuracy 0.5894\n",
      "step 16300 , validation loss : 41.4151\n",
      "step 16400 , training  accuracy 0.933333\n",
      "step 16400 , loss : 1.77441\n",
      "step 16400 , validation  accuracy 0.5964\n",
      "step 16400 , validation loss : 42.0445\n",
      "step 16500 , training  accuracy 0.8\n",
      "step 16500 , loss : 1.90231\n",
      "step 16500 , validation  accuracy 0.6034\n",
      "step 16500 , validation loss : 44.7781\n",
      "step 16600 , training  accuracy 0.9\n",
      "step 16600 , loss : 1.81581\n",
      "step 16600 , validation  accuracy 0.5956\n",
      "step 16600 , validation loss : 43.5093\n",
      "step 16700 , training  accuracy 0.866667\n",
      "step 16700 , loss : 1.80543\n",
      "step 16700 , validation  accuracy 0.5966\n",
      "step 16700 , validation loss : 43.5567\n",
      "step 16800 , training  accuracy 0.7\n",
      "step 16800 , loss : 2.01063\n",
      "step 16800 , validation  accuracy 0.5848\n",
      "step 16800 , validation loss : 44.521\n",
      "step 16900 , training  accuracy 0.666667\n",
      "step 16900 , loss : 1.99155\n",
      "step 16900 , validation  accuracy 0.5838\n",
      "step 16900 , validation loss : 44.0298\n",
      "step 17000 , training  accuracy 0.933333\n",
      "step 17000 , loss : 1.81783\n",
      "step 17000 , validation  accuracy 0.5858\n",
      "step 17000 , validation loss : 45.3385\n",
      "step 17100 , training  accuracy 0.866667\n",
      "step 17100 , loss : 1.83669\n",
      "step 17100 , validation  accuracy 0.5954\n",
      "step 17100 , validation loss : 42.5652\n",
      "step 17200 , training  accuracy 0.933333\n",
      "step 17200 , loss : 1.79416\n",
      "step 17200 , validation  accuracy 0.5862\n",
      "step 17200 , validation loss : 42.8243\n",
      "step 17300 , training  accuracy 0.7\n",
      "step 17300 , loss : 1.95784\n",
      "step 17300 , validation  accuracy 0.6012\n",
      "step 17300 , validation loss : 43.4652\n",
      "step 17400 , training  accuracy 0.866667\n",
      "step 17400 , loss : 1.8145\n",
      "step 17400 , validation  accuracy 0.5978\n",
      "step 17400 , validation loss : 42.6976\n",
      "step 17500 , training  accuracy 0.866667\n",
      "step 17500 , loss : 1.83049\n",
      "step 17500 , validation  accuracy 0.5956\n",
      "step 17500 , validation loss : 45.141\n",
      "step 17600 , training  accuracy 0.9\n",
      "step 17600 , loss : 1.81635\n",
      "step 17600 , validation  accuracy 0.6024\n",
      "step 17600 , validation loss : 44.7079\n",
      "step 17700 , training  accuracy 0.766667\n",
      "step 17700 , loss : 1.93889\n",
      "step 17700 , validation  accuracy 0.5996\n",
      "step 17700 , validation loss : 45.8862\n",
      "step 17800 , training  accuracy 0.733333\n",
      "step 17800 , loss : 1.94386\n",
      "step 17800 , validation  accuracy 0.6098\n",
      "step 17800 , validation loss : 46.3036\n",
      "step 17900 , training  accuracy 0.766667\n",
      "step 17900 , loss : 1.93728\n",
      "step 17900 , validation  accuracy 0.5958\n",
      "step 17900 , validation loss : 45.3529\n",
      "step 18000 , training  accuracy 0.766667\n",
      "step 18000 , loss : 1.91147\n",
      "step 18000 , validation  accuracy 0.5956\n",
      "step 18000 , validation loss : 45.673\n",
      "step 18100 , training  accuracy 0.9\n",
      "step 18100 , loss : 1.80069\n",
      "step 18100 , validation  accuracy 0.6018\n",
      "step 18100 , validation loss : 45.0948\n",
      "step 18200 , training  accuracy 0.666667\n",
      "step 18200 , loss : 1.92886\n",
      "step 18200 , validation  accuracy 0.5886\n",
      "step 18200 , validation loss : 43.6077\n",
      "step 18300 , training  accuracy 0.9\n",
      "step 18300 , loss : 1.8214\n",
      "step 18300 , validation  accuracy 0.6026\n",
      "step 18300 , validation loss : 44.6316\n",
      "step 18400 , training  accuracy 0.666667\n",
      "step 18400 , loss : 2.04069\n",
      "step 18400 , validation  accuracy 0.5862\n",
      "step 18400 , validation loss : 44.8891\n",
      "step 18500 , training  accuracy 0.833333\n",
      "step 18500 , loss : 1.84014\n",
      "step 18500 , validation  accuracy 0.5812\n",
      "step 18500 , validation loss : 43.8452\n",
      "step 18600 , training  accuracy 0.933333\n",
      "step 18600 , loss : 1.77528\n",
      "step 18600 , validation  accuracy 0.6042\n",
      "step 18600 , validation loss : 44.1638\n",
      "step 18700 , training  accuracy 0.666667\n",
      "step 18700 , loss : 1.96463\n",
      "step 18700 , validation  accuracy 0.609\n",
      "step 18700 , validation loss : 45.902\n",
      "step 18800 , training  accuracy 0.866667\n",
      "step 18800 , loss : 1.81499\n",
      "step 18800 , validation  accuracy 0.5888\n",
      "step 18800 , validation loss : 45.7088\n",
      "step 18900 , training  accuracy 0.8\n",
      "step 18900 , loss : 1.86427\n",
      "step 18900 , validation  accuracy 0.594\n",
      "step 18900 , validation loss : 43.3244\n",
      "step 19000 , training  accuracy 0.766667\n",
      "step 19000 , loss : 1.87001\n",
      "step 19000 , validation  accuracy 0.5996\n",
      "step 19000 , validation loss : 43.8757\n",
      "step 19100 , training  accuracy 0.8\n",
      "step 19100 , loss : 1.87993\n",
      "step 19100 , validation  accuracy 0.6074\n",
      "step 19100 , validation loss : 43.5491\n",
      "step 19200 , training  accuracy 0.766667\n",
      "step 19200 , loss : 1.87527\n",
      "step 19200 , validation  accuracy 0.6048\n",
      "step 19200 , validation loss : 46.1626\n",
      "step 19300 , training  accuracy 0.766667\n",
      "step 19300 , loss : 1.94423\n",
      "step 19300 , validation  accuracy 0.6036\n",
      "step 19300 , validation loss : 45.0518\n",
      "step 19400 , training  accuracy 0.666667\n",
      "step 19400 , loss : 1.98045\n",
      "step 19400 , validation  accuracy 0.5982\n",
      "step 19400 , validation loss : 45.4482\n",
      "step 19500 , training  accuracy 0.833333\n",
      "step 19500 , loss : 1.85501\n",
      "step 19500 , validation  accuracy 0.6104\n",
      "step 19500 , validation loss : 46.4403\n",
      "step 19600 , training  accuracy 0.9\n",
      "step 19600 , loss : 1.78685\n",
      "step 19600 , validation  accuracy 0.6002\n",
      "step 19600 , validation loss : 46.168\n",
      "step 19700 , training  accuracy 0.566667\n",
      "step 19700 , loss : 2.14867\n",
      "step 19700 , validation  accuracy 0.6048\n",
      "step 19700 , validation loss : 44.9178\n",
      "step 19800 , training  accuracy 0.666667\n",
      "step 19800 , loss : 2.04869\n",
      "step 19800 , validation  accuracy 0.5954\n",
      "step 19800 , validation loss : 46.6515\n",
      "step 19900 , training  accuracy 0.866667\n",
      "step 19900 , loss : 1.83374\n",
      "step 19900 , validation  accuracy 0.602\n",
      "step 19900 , validation loss : 45.1395\n",
      "step 20000 , training  accuracy 0.8\n",
      "step 20000 , loss : 1.87691\n",
      "step 20000 , validation  accuracy 0.6058\n",
      "step 20000 , validation loss : 43.9924\n",
      "step 20100 , training  accuracy 0.8\n",
      "step 20100 , loss : 1.89299\n",
      "step 20100 , validation  accuracy 0.6024\n",
      "step 20100 , validation loss : 45.8563\n",
      "step 20200 , training  accuracy 0.8\n",
      "step 20200 , loss : 1.85081\n",
      "step 20200 , validation  accuracy 0.5994\n",
      "step 20200 , validation loss : 45.5414\n",
      "step 20300 , training  accuracy 0.866667\n",
      "step 20300 , loss : 1.83738\n",
      "step 20300 , validation  accuracy 0.6034\n",
      "step 20300 , validation loss : 47.8994\n",
      "step 20400 , training  accuracy 0.933333\n",
      "step 20400 , loss : 1.79093\n",
      "step 20400 , validation  accuracy 0.5986\n",
      "step 20400 , validation loss : 47.6297\n",
      "step 20500 , training  accuracy 0.933333\n",
      "step 20500 , loss : 1.80287\n",
      "step 20500 , validation  accuracy 0.5902\n",
      "step 20500 , validation loss : 46.7411\n",
      "step 20600 , training  accuracy 0.9\n",
      "step 20600 , loss : 1.82394\n",
      "step 20600 , validation  accuracy 0.6088\n",
      "step 20600 , validation loss : 47.7901\n",
      "step 20700 , training  accuracy 0.966667\n",
      "step 20700 , loss : 1.76033\n",
      "step 20700 , validation  accuracy 0.6072\n",
      "step 20700 , validation loss : 47.7482\n",
      "step 20800 , training  accuracy 0.9\n",
      "step 20800 , loss : 1.77998\n",
      "step 20800 , validation  accuracy 0.601\n",
      "step 20800 , validation loss : 48.6851\n",
      "step 20900 , training  accuracy 0.766667\n",
      "step 20900 , loss : 1.91299\n",
      "step 20900 , validation  accuracy 0.6002\n",
      "step 20900 , validation loss : 45.835\n",
      "step 21000 , training  accuracy 0.7\n",
      "step 21000 , loss : 1.92981\n",
      "step 21000 , validation  accuracy 0.6018\n",
      "step 21000 , validation loss : 46.9324\n",
      "step 21100 , training  accuracy 0.933333\n",
      "step 21100 , loss : 1.74902\n",
      "step 21100 , validation  accuracy 0.5944\n",
      "step 21100 , validation loss : 47.8714\n",
      "step 21200 , training  accuracy 0.8\n",
      "step 21200 , loss : 1.89012\n",
      "step 21200 , validation  accuracy 0.603\n",
      "step 21200 , validation loss : 47.7816\n",
      "step 21300 , training  accuracy 0.833333\n",
      "step 21300 , loss : 1.83107\n",
      "step 21300 , validation  accuracy 0.5958\n",
      "step 21300 , validation loss : 47.007\n",
      "step 21400 , training  accuracy 0.8\n",
      "step 21400 , loss : 1.85681\n",
      "step 21400 , validation  accuracy 0.5912\n",
      "step 21400 , validation loss : 47.5009\n",
      "step 21500 , training  accuracy 0.833333\n",
      "step 21500 , loss : 1.80559\n",
      "step 21500 , validation  accuracy 0.6004\n",
      "step 21500 , validation loss : 47.779\n",
      "step 21600 , training  accuracy 0.8\n",
      "step 21600 , loss : 1.87534\n",
      "step 21600 , validation  accuracy 0.5954\n",
      "step 21600 , validation loss : 44.6844\n",
      "step 21700 , training  accuracy 0.966667\n",
      "step 21700 , loss : 1.74191\n",
      "step 21700 , validation  accuracy 0.5838\n",
      "step 21700 , validation loss : 47.9064\n",
      "step 21800 , training  accuracy 0.966667\n",
      "step 21800 , loss : 1.73086\n",
      "step 21800 , validation  accuracy 0.5952\n",
      "step 21800 , validation loss : 46.0684\n",
      "step 21900 , training  accuracy 0.9\n",
      "step 21900 , loss : 1.77886\n",
      "step 21900 , validation  accuracy 0.598\n",
      "step 21900 , validation loss : 47.8389\n",
      "step 22000 , training  accuracy 0.733333\n",
      "step 22000 , loss : 1.90739\n",
      "step 22000 , validation  accuracy 0.6006\n",
      "step 22000 , validation loss : 47.9769\n",
      "step 22100 , training  accuracy 0.8\n",
      "step 22100 , loss : 1.90749\n",
      "step 22100 , validation  accuracy 0.6012\n",
      "step 22100 , validation loss : 47.3918\n",
      "step 22200 , training  accuracy 0.9\n",
      "step 22200 , loss : 1.77227\n",
      "step 22200 , validation  accuracy 0.5908\n",
      "step 22200 , validation loss : 48.3864\n",
      "step 22300 , training  accuracy 0.9\n",
      "step 22300 , loss : 1.80716\n",
      "step 22300 , validation  accuracy 0.6016\n",
      "step 22300 , validation loss : 46.8205\n",
      "step 22400 , training  accuracy 0.866667\n",
      "step 22400 , loss : 1.82032\n",
      "step 22400 , validation  accuracy 0.5998\n",
      "step 22400 , validation loss : 48.4637\n",
      "step 22500 , training  accuracy 0.866667\n",
      "step 22500 , loss : 1.80659\n",
      "step 22500 , validation  accuracy 0.5908\n",
      "step 22500 , validation loss : 46.0588\n",
      "step 22600 , training  accuracy 0.733333\n",
      "step 22600 , loss : 1.95515\n",
      "step 22600 , validation  accuracy 0.5988\n",
      "step 22600 , validation loss : 46.3432\n",
      "step 22700 , training  accuracy 0.866667\n",
      "step 22700 , loss : 1.819\n",
      "step 22700 , validation  accuracy 0.6042\n",
      "step 22700 , validation loss : 49.6433\n",
      "step 22800 , training  accuracy 0.766667\n",
      "step 22800 , loss : 1.92608\n",
      "step 22800 , validation  accuracy 0.589\n",
      "step 22800 , validation loss : 48.8581\n",
      "step 22900 , training  accuracy 0.9\n",
      "step 22900 , loss : 1.81697\n",
      "step 22900 , validation  accuracy 0.5954\n",
      "step 22900 , validation loss : 46.953\n",
      "step 23000 , training  accuracy 0.966667\n",
      "step 23000 , loss : 1.75229\n",
      "step 23000 , validation  accuracy 0.6076\n",
      "step 23000 , validation loss : 48.758\n",
      "step 23100 , training  accuracy 0.833333\n",
      "step 23100 , loss : 1.86565\n",
      "step 23100 , validation  accuracy 0.5934\n",
      "step 23100 , validation loss : 47.6532\n",
      "step 23200 , training  accuracy 0.933333\n",
      "step 23200 , loss : 1.79561\n",
      "step 23200 , validation  accuracy 0.6056\n",
      "step 23200 , validation loss : 47.1043\n",
      "step 23300 , training  accuracy 0.7\n",
      "step 23300 , loss : 1.94406\n",
      "step 23300 , validation  accuracy 0.5982\n",
      "step 23300 , validation loss : 48.7356\n",
      "step 23400 , training  accuracy 0.966667\n",
      "step 23400 , loss : 1.7552\n",
      "step 23400 , validation  accuracy 0.6\n",
      "step 23400 , validation loss : 48.0236\n",
      "step 23500 , training  accuracy 0.933333\n",
      "step 23500 , loss : 1.76312\n",
      "step 23500 , validation  accuracy 0.6056\n",
      "step 23500 , validation loss : 47.7748\n",
      "step 23600 , training  accuracy 0.933333\n",
      "step 23600 , loss : 1.79864\n",
      "step 23600 , validation  accuracy 0.5928\n",
      "step 23600 , validation loss : 47.3198\n",
      "step 23700 , training  accuracy 0.8\n",
      "step 23700 , loss : 1.9001\n",
      "step 23700 , validation  accuracy 0.6076\n",
      "step 23700 , validation loss : 50.7195\n",
      "step 23800 , training  accuracy 0.933333\n",
      "step 23800 , loss : 1.77009\n",
      "step 23800 , validation  accuracy 0.6106\n",
      "step 23800 , validation loss : 48.3795\n",
      "step 23900 , training  accuracy 0.9\n",
      "step 23900 , loss : 1.81062\n",
      "step 23900 , validation  accuracy 0.5948\n",
      "step 23900 , validation loss : 48.5642\n",
      "step 24000 , training  accuracy 0.933333\n",
      "step 24000 , loss : 1.76228\n",
      "step 24000 , validation  accuracy 0.607\n",
      "step 24000 , validation loss : 49.631\n",
      "step 24100 , training  accuracy 0.966667\n",
      "step 24100 , loss : 1.73623\n",
      "step 24100 , validation  accuracy 0.6054\n",
      "step 24100 , validation loss : 47.0587\n",
      "step 24200 , training  accuracy 0.933333\n",
      "step 24200 , loss : 1.82375\n",
      "step 24200 , validation  accuracy 0.5868\n",
      "step 24200 , validation loss : 48.0648\n",
      "step 24300 , training  accuracy 0.833333\n",
      "step 24300 , loss : 1.9129\n",
      "step 24300 , validation  accuracy 0.6002\n",
      "step 24300 , validation loss : 49.7948\n",
      "step 24400 , training  accuracy 0.933333\n",
      "step 24400 , loss : 1.79887\n",
      "step 24400 , validation  accuracy 0.6044\n",
      "step 24400 , validation loss : 48.7296\n",
      "step 24500 , training  accuracy 0.933333\n",
      "step 24500 , loss : 1.75692\n",
      "step 24500 , validation  accuracy 0.5976\n",
      "step 24500 , validation loss : 48.1849\n",
      "step 24600 , training  accuracy 0.866667\n",
      "step 24600 , loss : 1.80642\n",
      "step 24600 , validation  accuracy 0.6062\n",
      "step 24600 , validation loss : 47.0316\n",
      "step 24700 , training  accuracy 0.866667\n",
      "step 24700 , loss : 1.81817\n",
      "step 24700 , validation  accuracy 0.6044\n",
      "step 24700 , validation loss : 50.4708\n",
      "step 24800 , training  accuracy 0.8\n",
      "step 24800 , loss : 1.90278\n",
      "step 24800 , validation  accuracy 0.5934\n",
      "step 24800 , validation loss : 49.3449\n",
      "step 24900 , training  accuracy 0.966667\n",
      "step 24900 , loss : 1.74471\n",
      "step 24900 , validation  accuracy 0.5998\n",
      "step 24900 , validation loss : 51.6346\n",
      "step 25000 , training  accuracy 0.966667\n",
      "step 25000 , loss : 1.73657\n",
      "step 25000 , validation  accuracy 0.61\n",
      "step 25000 , validation loss : 49.5922\n",
      "step 25100 , training  accuracy 0.8\n",
      "step 25100 , loss : 1.89791\n",
      "step 25100 , validation  accuracy 0.596\n",
      "step 25100 , validation loss : 48.9152\n",
      "step 25200 , training  accuracy 0.933333\n",
      "step 25200 , loss : 1.76032\n",
      "step 25200 , validation  accuracy 0.599\n",
      "step 25200 , validation loss : 51.4027\n",
      "step 25300 , training  accuracy 0.866667\n",
      "step 25300 , loss : 1.82036\n",
      "step 25300 , validation  accuracy 0.5924\n",
      "step 25300 , validation loss : 49.1406\n",
      "step 25400 , training  accuracy 0.933333\n",
      "step 25400 , loss : 1.77467\n",
      "step 25400 , validation  accuracy 0.6018\n",
      "step 25400 , validation loss : 50.0881\n",
      "step 25500 , training  accuracy 0.9\n",
      "step 25500 , loss : 1.82833\n",
      "step 25500 , validation  accuracy 0.5968\n",
      "step 25500 , validation loss : 50.4294\n",
      "step 25600 , training  accuracy 0.933333\n",
      "step 25600 , loss : 1.78581\n",
      "step 25600 , validation  accuracy 0.6124\n",
      "step 25600 , validation loss : 49.6494\n",
      "step 25700 , training  accuracy 0.866667\n",
      "step 25700 , loss : 1.80291\n",
      "step 25700 , validation  accuracy 0.5966\n",
      "step 25700 , validation loss : 50.458\n",
      "step 25800 , training  accuracy 0.833333\n",
      "step 25800 , loss : 1.83262\n",
      "step 25800 , validation  accuracy 0.6068\n",
      "step 25800 , validation loss : 51.6846\n",
      "step 25900 , training  accuracy 0.833333\n",
      "step 25900 , loss : 1.86526\n",
      "step 25900 , validation  accuracy 0.5922\n",
      "step 25900 , validation loss : 48.2688\n",
      "step 26000 , training  accuracy 0.966667\n",
      "step 26000 , loss : 1.72754\n",
      "step 26000 , validation  accuracy 0.597\n",
      "step 26000 , validation loss : 51.4479\n",
      "step 26100 , training  accuracy 0.933333\n",
      "step 26100 , loss : 1.80455\n",
      "step 26100 , validation  accuracy 0.5998\n",
      "step 26100 , validation loss : 49.4942\n",
      "step 26200 , training  accuracy 0.866667\n",
      "step 26200 , loss : 1.81376\n",
      "step 26200 , validation  accuracy 0.5972\n",
      "step 26200 , validation loss : 50.2764\n",
      "step 26300 , training  accuracy 0.9\n",
      "step 26300 , loss : 1.82072\n",
      "step 26300 , validation  accuracy 0.5982\n",
      "step 26300 , validation loss : 49.5252\n",
      "step 26400 , training  accuracy 0.866667\n",
      "step 26400 , loss : 1.81162\n",
      "step 26400 , validation  accuracy 0.6058\n",
      "step 26400 , validation loss : 50.0343\n",
      "step 26500 , training  accuracy 0.866667\n",
      "step 26500 , loss : 1.81087\n",
      "step 26500 , validation  accuracy 0.608\n",
      "step 26500 , validation loss : 52.9832\n",
      "step 26600 , training  accuracy 0.966667\n",
      "step 26600 , loss : 1.74292\n",
      "step 26600 , validation  accuracy 0.5926\n",
      "step 26600 , validation loss : 49.8705\n",
      "step 26700 , training  accuracy 0.9\n",
      "step 26700 , loss : 1.78024\n",
      "step 26700 , validation  accuracy 0.6084\n",
      "step 26700 , validation loss : 51.5045\n",
      "step 26800 , training  accuracy 0.866667\n",
      "step 26800 , loss : 1.81655\n",
      "step 26800 , validation  accuracy 0.5976\n",
      "step 26800 , validation loss : 52.28\n",
      "step 26900 , training  accuracy 0.866667\n",
      "step 26900 , loss : 1.81057\n",
      "step 26900 , validation  accuracy 0.5944\n",
      "step 26900 , validation loss : 52.168\n",
      "step 27000 , training  accuracy 0.966667\n",
      "step 27000 , loss : 1.73516\n",
      "step 27000 , validation  accuracy 0.599\n",
      "step 27000 , validation loss : 50.2189\n",
      "step 27100 , training  accuracy 0.866667\n",
      "step 27100 , loss : 1.82923\n",
      "step 27100 , validation  accuracy 0.6046\n",
      "step 27100 , validation loss : 49.5977\n",
      "step 27200 , training  accuracy 0.833333\n",
      "step 27200 , loss : 1.83791\n",
      "step 27200 , validation  accuracy 0.6092\n",
      "step 27200 , validation loss : 50.1441\n",
      "step 27300 , training  accuracy 0.833333\n",
      "step 27300 , loss : 1.88011\n",
      "step 27300 , validation  accuracy 0.6006\n",
      "step 27300 , validation loss : 51.0451\n",
      "step 27400 , training  accuracy 0.9\n",
      "step 27400 , loss : 1.79346\n",
      "step 27400 , validation  accuracy 0.597\n",
      "step 27400 , validation loss : 50.2978\n",
      "step 27500 , training  accuracy 0.966667\n",
      "step 27500 , loss : 1.76211\n",
      "step 27500 , validation  accuracy 0.6036\n",
      "step 27500 , validation loss : 51.653\n",
      "step 27600 , training  accuracy 0.933333\n",
      "step 27600 , loss : 1.81364\n",
      "step 27600 , validation  accuracy 0.6052\n",
      "step 27600 , validation loss : 51.2691\n",
      "step 27700 , training  accuracy 0.8\n",
      "step 27700 , loss : 1.8561\n",
      "step 27700 , validation  accuracy 0.5982\n",
      "step 27700 , validation loss : 50.5736\n",
      "step 27800 , training  accuracy 0.833333\n",
      "step 27800 , loss : 1.85549\n",
      "step 27800 , validation  accuracy 0.6084\n",
      "step 27800 , validation loss : 50.3289\n",
      "step 27900 , training  accuracy 0.9\n",
      "step 27900 , loss : 1.7659\n",
      "step 27900 , validation  accuracy 0.6146\n",
      "step 27900 , validation loss : 50.7127\n",
      "step 28000 , training  accuracy 0.933333\n",
      "step 28000 , loss : 1.74862\n",
      "step 28000 , validation  accuracy 0.6108\n",
      "step 28000 , validation loss : 51.4298\n",
      "step 28100 , training  accuracy 0.933333\n",
      "step 28100 , loss : 1.77565\n",
      "step 28100 , validation  accuracy 0.6006\n",
      "step 28100 , validation loss : 48.4979\n",
      "step 28200 , training  accuracy 0.866667\n",
      "step 28200 , loss : 1.86014\n",
      "step 28200 , validation  accuracy 0.598\n",
      "step 28200 , validation loss : 50.5325\n",
      "step 28300 , training  accuracy 0.933333\n",
      "step 28300 , loss : 1.76719\n",
      "step 28300 , validation  accuracy 0.6\n",
      "step 28300 , validation loss : 50.4859\n",
      "step 28400 , training  accuracy 1\n",
      "step 28400 , loss : 1.73421\n",
      "step 28400 , validation  accuracy 0.6014\n",
      "step 28400 , validation loss : 51.4118\n",
      "step 28500 , training  accuracy 0.966667\n",
      "step 28500 , loss : 1.73159\n",
      "step 28500 , validation  accuracy 0.5968\n",
      "step 28500 , validation loss : 52.112\n",
      "step 28600 , training  accuracy 0.933333\n",
      "step 28600 , loss : 1.78317\n",
      "step 28600 , validation  accuracy 0.5892\n",
      "step 28600 , validation loss : 51.2357\n",
      "step 28700 , training  accuracy 1\n",
      "step 28700 , loss : 1.72293\n",
      "step 28700 , validation  accuracy 0.6004\n",
      "step 28700 , validation loss : 51.993\n",
      "step 28800 , training  accuracy 0.966667\n",
      "step 28800 , loss : 1.7197\n",
      "step 28800 , validation  accuracy 0.6048\n",
      "step 28800 , validation loss : 49.7244\n",
      "step 28900 , training  accuracy 0.966667\n",
      "step 28900 , loss : 1.74907\n",
      "step 28900 , validation  accuracy 0.6092\n",
      "step 28900 , validation loss : 50.3378\n",
      "step 29000 , training  accuracy 0.933333\n",
      "step 29000 , loss : 1.76951\n",
      "step 29000 , validation  accuracy 0.6022\n",
      "step 29000 , validation loss : 53.7386\n",
      "step 29100 , training  accuracy 0.966667\n",
      "step 29100 , loss : 1.73313\n",
      "step 29100 , validation  accuracy 0.6104\n",
      "step 29100 , validation loss : 51.6959\n",
      "step 29200 , training  accuracy 0.9\n",
      "step 29200 , loss : 1.79967\n",
      "step 29200 , validation  accuracy 0.6034\n",
      "step 29200 , validation loss : 51.5228\n",
      "step 29300 , training  accuracy 0.8\n",
      "step 29300 , loss : 1.86547\n",
      "step 29300 , validation  accuracy 0.5972\n",
      "step 29300 , validation loss : 51.72\n",
      "step 29400 , training  accuracy 0.8\n",
      "step 29400 , loss : 1.87199\n",
      "step 29400 , validation  accuracy 0.6076\n",
      "step 29400 , validation loss : 51.1886\n",
      "step 29500 , training  accuracy 0.833333\n",
      "step 29500 , loss : 1.85639\n",
      "step 29500 , validation  accuracy 0.6084\n",
      "step 29500 , validation loss : 52.3979\n",
      "step 29600 , training  accuracy 0.833333\n",
      "step 29600 , loss : 1.84515\n",
      "step 29600 , validation  accuracy 0.5964\n",
      "step 29600 , validation loss : 52.4191\n",
      "step 29700 , training  accuracy 1\n",
      "step 29700 , loss : 1.72233\n",
      "step 29700 , validation  accuracy 0.6126\n",
      "step 29700 , validation loss : 51.457\n",
      "step 29800 , training  accuracy 0.9\n",
      "step 29800 , loss : 1.80132\n",
      "step 29800 , validation  accuracy 0.605\n",
      "step 29800 , validation loss : 52.8522\n",
      "step 29900 , training  accuracy 0.766667\n",
      "step 29900 , loss : 1.91576\n",
      "step 29900 , validation  accuracy 0.598\n",
      "step 29900 , validation loss : 51.9804\n",
      "step 30000 , training  accuracy 0.966667\n",
      "step 30000 , loss : 1.75222\n",
      "step 30000 , validation  accuracy 0.5996\n",
      "step 30000 , validation loss : 53.8781\n",
      "step 30100 , training  accuracy 0.9\n",
      "step 30100 , loss : 1.80007\n",
      "step 30100 , validation  accuracy 0.6048\n",
      "step 30100 , validation loss : 52.6109\n",
      "step 30200 , training  accuracy 0.9\n",
      "step 30200 , loss : 1.81129\n",
      "step 30200 , validation  accuracy 0.6028\n",
      "step 30200 , validation loss : 51.1824\n",
      "step 30300 , training  accuracy 0.866667\n",
      "step 30300 , loss : 1.83891\n",
      "step 30300 , validation  accuracy 0.6076\n",
      "step 30300 , validation loss : 52.7674\n",
      "step 30400 , training  accuracy 0.933333\n",
      "step 30400 , loss : 1.78475\n",
      "step 30400 , validation  accuracy 0.5954\n",
      "step 30400 , validation loss : 51.9501\n",
      "step 30500 , training  accuracy 0.966667\n",
      "step 30500 , loss : 1.74717\n",
      "step 30500 , validation  accuracy 0.599\n",
      "step 30500 , validation loss : 52.1029\n",
      "step 30600 , training  accuracy 0.966667\n",
      "step 30600 , loss : 1.76267\n",
      "step 30600 , validation  accuracy 0.6006\n",
      "step 30600 , validation loss : 52.324\n",
      "step 30700 , training  accuracy 0.966667\n",
      "step 30700 , loss : 1.74699\n",
      "step 30700 , validation  accuracy 0.6032\n",
      "step 30700 , validation loss : 53.8927\n",
      "step 30800 , training  accuracy 0.933333\n",
      "step 30800 , loss : 1.79008\n",
      "step 30800 , validation  accuracy 0.5948\n",
      "step 30800 , validation loss : 53.5726\n",
      "step 30900 , training  accuracy 0.966667\n",
      "step 30900 , loss : 1.76229\n",
      "step 30900 , validation  accuracy 0.5992\n",
      "step 30900 , validation loss : 53.6745\n",
      "step 31000 , training  accuracy 0.966667\n",
      "step 31000 , loss : 1.75924\n",
      "step 31000 , validation  accuracy 0.607\n",
      "step 31000 , validation loss : 53.9072\n",
      "step 31100 , training  accuracy 0.9\n",
      "step 31100 , loss : 1.80825\n",
      "step 31100 , validation  accuracy 0.6018\n",
      "step 31100 , validation loss : 53.6092\n",
      "step 31200 , training  accuracy 0.933333\n",
      "step 31200 , loss : 1.76086\n",
      "step 31200 , validation  accuracy 0.5998\n",
      "step 31200 , validation loss : 51.6289\n",
      "step 31300 , training  accuracy 0.933333\n",
      "step 31300 , loss : 1.7406\n",
      "step 31300 , validation  accuracy 0.5936\n",
      "step 31300 , validation loss : 52.1219\n",
      "step 31400 , training  accuracy 0.9\n",
      "step 31400 , loss : 1.78532\n",
      "step 31400 , validation  accuracy 0.6018\n",
      "step 31400 , validation loss : 50.9504\n",
      "step 31500 , training  accuracy 0.933333\n",
      "step 31500 , loss : 1.7747\n",
      "step 31500 , validation  accuracy 0.5998\n",
      "step 31500 , validation loss : 53.0542\n",
      "step 31600 , training  accuracy 0.933333\n",
      "step 31600 , loss : 1.76862\n",
      "step 31600 , validation  accuracy 0.5932\n",
      "step 31600 , validation loss : 53.2579\n",
      "step 31700 , training  accuracy 0.966667\n",
      "step 31700 , loss : 1.72689\n",
      "step 31700 , validation  accuracy 0.598\n",
      "step 31700 , validation loss : 50.8029\n",
      "step 31800 , training  accuracy 0.833333\n",
      "step 31800 , loss : 1.87717\n",
      "step 31800 , validation  accuracy 0.5952\n",
      "step 31800 , validation loss : 52.8091\n",
      "step 31900 , training  accuracy 0.833333\n",
      "step 31900 , loss : 1.87915\n",
      "step 31900 , validation  accuracy 0.5968\n",
      "step 31900 , validation loss : 53.0484\n",
      "step 32000 , training  accuracy 0.933333\n",
      "step 32000 , loss : 1.77246\n",
      "step 32000 , validation  accuracy 0.6062\n",
      "step 32000 , validation loss : 55.0048\n",
      "step 32100 , training  accuracy 1\n",
      "step 32100 , loss : 1.70271\n",
      "step 32100 , validation  accuracy 0.5886\n",
      "step 32100 , validation loss : 52.5432\n",
      "step 32200 , training  accuracy 0.9\n",
      "step 32200 , loss : 1.79204\n",
      "step 32200 , validation  accuracy 0.5976\n",
      "step 32200 , validation loss : 55.2652\n",
      "step 32300 , training  accuracy 0.933333\n",
      "step 32300 , loss : 1.76178\n",
      "step 32300 , validation  accuracy 0.5996\n",
      "step 32300 , validation loss : 53.5936\n",
      "step 32400 , training  accuracy 1\n",
      "step 32400 , loss : 1.70607\n",
      "step 32400 , validation  accuracy 0.5948\n",
      "step 32400 , validation loss : 53.6772\n",
      "step 32500 , training  accuracy 0.866667\n",
      "step 32500 , loss : 1.80477\n",
      "step 32500 , validation  accuracy 0.5986\n",
      "step 32500 , validation loss : 52.8113\n",
      "step 32600 , training  accuracy 0.933333\n",
      "step 32600 , loss : 1.75961\n",
      "step 32600 , validation  accuracy 0.5944\n",
      "step 32600 , validation loss : 52.8606\n",
      "step 32700 , training  accuracy 0.966667\n",
      "step 32700 , loss : 1.72361\n",
      "step 32700 , validation  accuracy 0.603\n",
      "step 32700 , validation loss : 52.3903\n",
      "step 32800 , training  accuracy 0.966667\n",
      "step 32800 , loss : 1.73663\n",
      "step 32800 , validation  accuracy 0.593\n",
      "step 32800 , validation loss : 53.5243\n",
      "step 32900 , training  accuracy 0.933333\n",
      "step 32900 , loss : 1.74799\n",
      "step 32900 , validation  accuracy 0.5956\n",
      "step 32900 , validation loss : 54.6155\n",
      "step 33000 , training  accuracy 0.966667\n",
      "step 33000 , loss : 1.72735\n",
      "step 33000 , validation  accuracy 0.5968\n",
      "step 33000 , validation loss : 53.1833\n",
      "step 33100 , training  accuracy 0.933333\n",
      "step 33100 , loss : 1.77632\n",
      "step 33100 , validation  accuracy 0.5998\n",
      "step 33100 , validation loss : 54.0404\n",
      "step 33200 , training  accuracy 1\n",
      "step 33200 , loss : 1.70482\n",
      "step 33200 , validation  accuracy 0.6006\n",
      "step 33200 , validation loss : 53.4004\n",
      "step 33300 , training  accuracy 0.833333\n",
      "step 33300 , loss : 1.88428\n",
      "step 33300 , validation  accuracy 0.5898\n",
      "step 33300 , validation loss : 54.9245\n",
      "step 33400 , training  accuracy 1\n",
      "step 33400 , loss : 1.69782\n",
      "step 33400 , validation  accuracy 0.5972\n",
      "step 33400 , validation loss : 54.8703\n",
      "step 33500 , training  accuracy 0.933333\n",
      "step 33500 , loss : 1.76621\n",
      "step 33500 , validation  accuracy 0.5854\n",
      "step 33500 , validation loss : 54.4086\n",
      "step 33600 , training  accuracy 0.866667\n",
      "step 33600 , loss : 1.82436\n",
      "step 33600 , validation  accuracy 0.5968\n",
      "step 33600 , validation loss : 53.8056\n",
      "step 33700 , training  accuracy 0.9\n",
      "step 33700 , loss : 1.80899\n",
      "step 33700 , validation  accuracy 0.601\n",
      "step 33700 , validation loss : 54.9296\n",
      "step 33800 , training  accuracy 0.966667\n",
      "step 33800 , loss : 1.7318\n",
      "step 33800 , validation  accuracy 0.605\n",
      "step 33800 , validation loss : 54.1426\n",
      "step 33900 , training  accuracy 0.866667\n",
      "step 33900 , loss : 1.8183\n",
      "step 33900 , validation  accuracy 0.6004\n",
      "step 33900 , validation loss : 53.3582\n",
      "step 34000 , training  accuracy 0.933333\n",
      "step 34000 , loss : 1.75871\n",
      "step 34000 , validation  accuracy 0.6024\n",
      "step 34000 , validation loss : 54.4994\n",
      "step 34100 , training  accuracy 0.866667\n",
      "step 34100 , loss : 1.82129\n",
      "step 34100 , validation  accuracy 0.6074\n",
      "step 34100 , validation loss : 55.8345\n",
      "step 34200 , training  accuracy 0.966667\n",
      "step 34200 , loss : 1.72454\n",
      "step 34200 , validation  accuracy 0.6086\n",
      "step 34200 , validation loss : 53.4391\n",
      "step 34300 , training  accuracy 1\n",
      "step 34300 , loss : 1.71102\n",
      "step 34300 , validation  accuracy 0.5858\n",
      "step 34300 , validation loss : 53.2153\n",
      "step 34400 , training  accuracy 0.9\n",
      "step 34400 , loss : 1.78218\n",
      "step 34400 , validation  accuracy 0.5892\n",
      "step 34400 , validation loss : 51.8847\n",
      "step 34500 , training  accuracy 0.933333\n",
      "step 34500 , loss : 1.75253\n",
      "step 34500 , validation  accuracy 0.6128\n",
      "step 34500 , validation loss : 55.3921\n",
      "step 34600 , training  accuracy 0.966667\n",
      "step 34600 , loss : 1.74797\n",
      "step 34600 , validation  accuracy 0.5998\n",
      "step 34600 , validation loss : 52.7043\n",
      "step 34700 , training  accuracy 0.933333\n",
      "step 34700 , loss : 1.75193\n",
      "step 34700 , validation  accuracy 0.6026\n",
      "step 34700 , validation loss : 53.6616\n",
      "step 34800 , training  accuracy 0.866667\n",
      "step 34800 , loss : 1.86468\n",
      "step 34800 , validation  accuracy 0.5974\n",
      "step 34800 , validation loss : 54.4266\n",
      "step 34900 , training  accuracy 1\n",
      "step 34900 , loss : 1.71059\n",
      "step 34900 , validation  accuracy 0.6056\n",
      "step 34900 , validation loss : 56.0658\n",
      "step 35000 , training  accuracy 1\n",
      "step 35000 , loss : 1.73266\n",
      "step 35000 , validation  accuracy 0.6016\n",
      "step 35000 , validation loss : 54.465\n",
      "step 35100 , training  accuracy 0.933333\n",
      "step 35100 , loss : 1.75973\n",
      "step 35100 , validation  accuracy 0.601\n",
      "step 35100 , validation loss : 56.1189\n",
      "step 35200 , training  accuracy 0.8\n",
      "step 35200 , loss : 1.81002\n",
      "step 35200 , validation  accuracy 0.602\n",
      "step 35200 , validation loss : 54.4676\n",
      "step 35300 , training  accuracy 0.966667\n",
      "step 35300 , loss : 1.73958\n",
      "step 35300 , validation  accuracy 0.599\n",
      "step 35300 , validation loss : 55.188\n",
      "step 35400 , training  accuracy 0.966667\n",
      "step 35400 , loss : 1.72845\n",
      "step 35400 , validation  accuracy 0.6064\n",
      "step 35400 , validation loss : 56.5645\n",
      "step 35500 , training  accuracy 0.833333\n",
      "step 35500 , loss : 1.85934\n",
      "step 35500 , validation  accuracy 0.6048\n",
      "step 35500 , validation loss : 54.8408\n",
      "step 35600 , training  accuracy 0.966667\n",
      "step 35600 , loss : 1.74801\n",
      "step 35600 , validation  accuracy 0.5954\n",
      "step 35600 , validation loss : 54.9422\n",
      "step 35700 , training  accuracy 0.933333\n",
      "step 35700 , loss : 1.75587\n",
      "step 35700 , validation  accuracy 0.6054\n",
      "step 35700 , validation loss : 55.4972\n",
      "step 35800 , training  accuracy 0.9\n",
      "step 35800 , loss : 1.79053\n",
      "step 35800 , validation  accuracy 0.6036\n",
      "step 35800 , validation loss : 55.3785\n",
      "step 35900 , training  accuracy 0.933333\n",
      "step 35900 , loss : 1.79635\n",
      "step 35900 , validation  accuracy 0.6144\n",
      "step 35900 , validation loss : 54.3773\n",
      "step 36000 , training  accuracy 0.933333\n",
      "step 36000 , loss : 1.79123\n",
      "step 36000 , validation  accuracy 0.612\n",
      "step 36000 , validation loss : 53.6564\n",
      "step 36100 , training  accuracy 0.966667\n",
      "step 36100 , loss : 1.74605\n",
      "step 36100 , validation  accuracy 0.606\n",
      "step 36100 , validation loss : 54.5128\n",
      "step 36200 , training  accuracy 0.9\n",
      "step 36200 , loss : 1.80575\n",
      "step 36200 , validation  accuracy 0.6088\n",
      "step 36200 , validation loss : 56.1933\n",
      "step 36300 , training  accuracy 0.9\n",
      "step 36300 , loss : 1.80233\n",
      "step 36300 , validation  accuracy 0.5998\n",
      "step 36300 , validation loss : 56.3303\n",
      "step 36400 , training  accuracy 0.966667\n",
      "step 36400 , loss : 1.74385\n",
      "step 36400 , validation  accuracy 0.5916\n",
      "step 36400 , validation loss : 55.4179\n",
      "step 36500 , training  accuracy 0.933333\n",
      "step 36500 , loss : 1.76895\n",
      "step 36500 , validation  accuracy 0.6044\n",
      "step 36500 , validation loss : 54.6668\n",
      "step 36600 , training  accuracy 0.966667\n",
      "step 36600 , loss : 1.7211\n",
      "step 36600 , validation  accuracy 0.6102\n",
      "step 36600 , validation loss : 53.5119\n",
      "step 36700 , training  accuracy 1\n",
      "step 36700 , loss : 1.70271\n",
      "step 36700 , validation  accuracy 0.6018\n",
      "step 36700 , validation loss : 55.9693\n",
      "step 36800 , training  accuracy 0.966667\n",
      "step 36800 , loss : 1.7162\n",
      "step 36800 , validation  accuracy 0.5964\n",
      "step 36800 , validation loss : 54.7115\n",
      "step 36900 , training  accuracy 0.933333\n",
      "step 36900 , loss : 1.76004\n",
      "step 36900 , validation  accuracy 0.602\n",
      "step 36900 , validation loss : 57.0104\n",
      "step 37000 , training  accuracy 0.833333\n",
      "step 37000 , loss : 1.87805\n",
      "step 37000 , validation  accuracy 0.5922\n",
      "step 37000 , validation loss : 55.5222\n",
      "step 37100 , training  accuracy 1\n",
      "step 37100 , loss : 1.70384\n",
      "step 37100 , validation  accuracy 0.601\n",
      "step 37100 , validation loss : 56.0742\n",
      "step 37200 , training  accuracy 1\n",
      "step 37200 , loss : 1.73265\n",
      "step 37200 , validation  accuracy 0.5922\n",
      "step 37200 , validation loss : 56.302\n",
      "step 37300 , training  accuracy 1\n",
      "step 37300 , loss : 1.71142\n",
      "step 37300 , validation  accuracy 0.608\n",
      "step 37300 , validation loss : 56.141\n",
      "step 37400 , training  accuracy 0.933333\n",
      "step 37400 , loss : 1.74831\n",
      "step 37400 , validation  accuracy 0.6036\n",
      "step 37400 , validation loss : 56.2\n",
      "step 37500 , training  accuracy 0.833333\n",
      "step 37500 , loss : 1.81898\n",
      "step 37500 , validation  accuracy 0.5966\n",
      "step 37500 , validation loss : 56.0276\n",
      "step 37600 , training  accuracy 0.9\n",
      "step 37600 , loss : 1.8093\n",
      "step 37600 , validation  accuracy 0.5906\n",
      "step 37600 , validation loss : 54.1524\n",
      "step 37700 , training  accuracy 0.866667\n",
      "step 37700 , loss : 1.81376\n",
      "step 37700 , validation  accuracy 0.603\n",
      "step 37700 , validation loss : 54.863\n",
      "step 37800 , training  accuracy 0.9\n",
      "step 37800 , loss : 1.79493\n",
      "step 37800 , validation  accuracy 0.6038\n",
      "step 37800 , validation loss : 55.8566\n",
      "step 37900 , training  accuracy 0.933333\n",
      "step 37900 , loss : 1.77696\n",
      "step 37900 , validation  accuracy 0.6026\n",
      "step 37900 , validation loss : 54.974\n",
      "step 38000 , training  accuracy 0.966667\n",
      "step 38000 , loss : 1.76856\n",
      "step 38000 , validation  accuracy 0.6094\n",
      "step 38000 , validation loss : 55.2468\n",
      "step 38100 , training  accuracy 0.933333\n",
      "step 38100 , loss : 1.75912\n",
      "step 38100 , validation  accuracy 0.6038\n",
      "step 38100 , validation loss : 54.2852\n",
      "step 38200 , training  accuracy 0.933333\n",
      "step 38200 , loss : 1.77491\n",
      "step 38200 , validation  accuracy 0.6\n",
      "step 38200 , validation loss : 55.9031\n",
      "step 38300 , training  accuracy 0.866667\n",
      "step 38300 , loss : 1.84651\n",
      "step 38300 , validation  accuracy 0.5958\n",
      "step 38300 , validation loss : 55.954\n",
      "step 38400 , training  accuracy 0.9\n",
      "step 38400 , loss : 1.81502\n",
      "step 38400 , validation  accuracy 0.5944\n",
      "step 38400 , validation loss : 55.4124\n",
      "step 38500 , training  accuracy 0.866667\n",
      "step 38500 , loss : 1.79041\n",
      "step 38500 , validation  accuracy 0.6028\n",
      "step 38500 , validation loss : 56.0783\n",
      "step 38600 , training  accuracy 0.966667\n",
      "step 38600 , loss : 1.75057\n",
      "step 38600 , validation  accuracy 0.5904\n",
      "step 38600 , validation loss : 56.3987\n",
      "step 38700 , training  accuracy 0.966667\n",
      "step 38700 , loss : 1.72978\n",
      "step 38700 , validation  accuracy 0.5978\n",
      "step 38700 , validation loss : 54.522\n",
      "step 38800 , training  accuracy 1\n",
      "step 38800 , loss : 1.70006\n",
      "step 38800 , validation  accuracy 0.6022\n",
      "step 38800 , validation loss : 56.5067\n",
      "step 38900 , training  accuracy 0.9\n",
      "step 38900 , loss : 1.8167\n",
      "step 38900 , validation  accuracy 0.6036\n",
      "step 38900 , validation loss : 56.9449\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "#sm_conv= tf.nn.softmax(y_conv)\n",
    "    #cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "    start_time = time.time()\n",
    "\n",
    "    regular=0.01*(tf.reduce_sum(tf.square(y_conv)))\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( y_conv, y_))\n",
    "with tf.device('/gpu:0'):\n",
    "    cost = cost+regular\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cost) #1e-4\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y_conv,1) ,tf.argmax(y_,1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction , \"float\")) \n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "if divide_flag ==True:\n",
    "    n_batch =len(train_images)\n",
    "    batch_count=0\n",
    "\n",
    "for i in range(iterate):    \n",
    "    if divide_flag ==True:\n",
    "        if batch_count >= n_batch:\n",
    "            batch_count =0\n",
    "        train_img =np.load(file_locate+train_images[batch_count])\n",
    "    \n",
    "        train_lab =np.load(file_locate+train_labels[batch_count])\n",
    "    batch_xs , batch_ys = next_batch(batch_size, train_img , train_lab)\n",
    "   # batch_val_xs  , batch_val_ys = next_batch(20 , val_img , val_lab)\n",
    "    if i%100 ==0: # in here add to validation \n",
    "        try:\n",
    "            val_accuracy = sess.run( accuracy , feed_dict={x:val_img , y_:val_lab , keep_prob: 1.0})        \n",
    "            val_loss = sess.run(cost , feed_dict = {x:val_img , y_: val_lab , keep_prob: 1.0})\n",
    "            \n",
    "            train_accuracy = sess.run( accuracy , feed_dict={x:batch_xs , y_:batch_ys , keep_prob: 1.0})        \n",
    "            train_loss = sess.run(cost , feed_dict = {x:batch_xs, y_: batch_ys, keep_prob: 1.0})\n",
    "\n",
    "            #result = sess.run(sm_conv , feed_dict = {x:val_img , y_:batch_ys , keep_prob :1.0})\n",
    "            print(\"step %d , training  accuracy %g\" %(i,train_accuracy))\n",
    "            print(\"step %d , loss : %g\" %(i,train_loss))\n",
    "            train_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(train_loss) +'\\tval accuracy:\\t'+str(train_accuracy)+'\\n'\n",
    "          \n",
    "            print(\"step %d , validation  accuracy %g\" %(i,val_accuracy))\n",
    "            print(\"step %d , validation loss : %g\" %(i,val_loss))\n",
    "            val_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(val_loss) +'\\tval accuracy:\\t'+str(val_accuracy)+'\\n'\n",
    "            \n",
    "            \n",
    "            f.write(val_str)\n",
    "            f.write(train_str)\n",
    "            batch_count+=1\n",
    "        except :\n",
    "            list_acc=[]\n",
    "            list_loss=[]\n",
    "            n_divide=len(val_img)/batch_size\n",
    "            j=0\n",
    "            for j in range(n_divide):\n",
    "                \n",
    "                # j*batch_size :(j+1)*batch_size\n",
    "                val_accuracy,val_loss = sess.run([accuracy ,cost], feed_dict={x:val_img[ j*batch_size :(j+1)*batch_size] , y_:val_lab[ j*batch_size :(j+1)*batch_size ] , keep_prob: 1.0})        \n",
    "                list_acc.append(float(val_accuracy))\n",
    "                list_loss.append(float(val_loss))\n",
    "            #right above code have to modify\n",
    "            val_accuracy,val_loss = sess.run([accuracy ,cost], feed_dict={x:val_img[ j*batch_size :] , y_:val_lab[ j*batch_size :  ] , keep_prob: 1.0})         \n",
    "            list_acc.append(val_accuracy)\n",
    "            list_loss.append(val_loss)\n",
    "            \n",
    "            list_acc=np.asarray(list_acc)\n",
    "            list_loss= np.asarray(list_loss)\n",
    "            \n",
    "            val_accuracy=np.mean(list_acc)\n",
    "            val_loss = np.mean(list_loss)\n",
    "            \n",
    "            #result = sess.run(sm_conv , feed_dict = {x:val_img , y_:batch_ys , keep_prob :1.0})\n",
    "            \n",
    "            train_accuracy = sess.run( accuracy , feed_dict={x:batch_xs , y_:batch_ys , keep_prob: 1.0})        \n",
    "            train_loss = sess.run(cost , feed_dict = {x:batch_xs, y_: batch_ys, keep_prob: 1.0})\n",
    "\n",
    "            print(\"step %d , training  accuracy %g\" %(i,train_accuracy))\n",
    "            print(\"step %d , loss : %g\" %(i,train_loss))\n",
    "            train_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(train_loss) +'\\tval accuracy:\\t'+str(train_accuracy)+'\\n'\n",
    "            \n",
    "            print(\"step %d , validation  accuracy %g\" %(i,val_accuracy))\n",
    "            print(\"step %d , validation loss : %g\" %(i,val_loss))\n",
    "            val_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(val_loss) +'\\tval accuracy:\\t'+str(val_accuracy)+'\\n'\n",
    "           \n",
    "            \n",
    "            f.write(val_str)\n",
    "            f.write(train_str)\n",
    "            batch_count+=1\n",
    "    \n",
    "    sess.run(train_step ,feed_dict={x:batch_xs , y_:batch_ys , keep_prob : 0.7})\n",
    "    \n",
    "print(\"--- Training Time : %s ---\" % (time.time() - start_time))\n",
    "train_time=\"--- Training Time : ---:\\t\" +str(time.time() - start_time)\n",
    "f.write(train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print ''\n",
    "print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_accuracy = sess.run( accuracy , feed_dict={x:test_img , y_:test_lab , keep_prob: 1.0})        \n",
    "    test_loss = sess.run(cost , feed_dict = {x:test_img , y_: test_lab , keep_prob: 1.0})\n",
    "\n",
    "    #result = sess.run(sm_conv , feed_dict = {x:test_img , y_:batch_ys , keep_prob :1.0})\n",
    "    print(\"step %d , testidation  accuracy %g\" %(i,test_accuracy))\n",
    "    print(\"step %d , testidation loss : %g\" %(i,test_loss))\n",
    "    test_str = 'step:\\t'+str(i)+'\\ttest_loss:\\t'+str(test_loss) +'\\ttest accuracy:\\t'+str(test_accuracy)+'\\n'\n",
    "\n",
    "    f.write(test_str)\n",
    "except :\n",
    "    list_acc=[]\n",
    "    list_loss=[]\n",
    "    n_divide=len(test_img)/batch_size\n",
    "    for j in range(n_divide):\n",
    "\n",
    "        # j*batch_size :(j+1)*batch_size\n",
    "        test_accuracy,test_loss = sess.run([accuracy ,cost], feed_dict={x:test_img[ j*batch_size :(j+1)*batch_size] , y_:test_lab[ j*batch_size :(j+1)*batch_size ] , keep_prob: 1.0})        \n",
    "        list_acc.append(float(test_accuracy))\n",
    "        list_loss.append(float(test_loss))\n",
    "    test_accuracy , test_loss=sess.run([accuracy,cost] , feed_dict={x:test_img[(j+1)*batch_size : ] , y_:test_lab[(j+1)*(batch_size) : ] , keep_prob : 1.0})\n",
    "    #right above code have to modify\n",
    "\n",
    "    list_acc.append(test_accuracy)\n",
    "    list_loss.append(test_loss)\n",
    "    list_acc=np.asarray(list_acc)\n",
    "    list_loss= np.asarray(list_loss)\n",
    "\n",
    "    test_accuracy=np.mean(list_acc)\n",
    "    test_loss = np.mean(list_loss)\n",
    "\n",
    "    #result = sess.run(sm_conv , feed_dict = {x:test_img , y_:batch_ys , keep_prob :1.0})\n",
    "    print(\"step %d , testidation  accuracy %g\" %(i,test_accuracy))\n",
    "    print(\"step %d , testidation loss : %g\" %(i,test_loss))\n",
    "    test_str = 'step:\\t'+str(i)+'\\ttest_loss:\\t'+str(test_loss) +'\\ttest accuracy:\\t'+str(test_accuracy)+'\\n'\n",
    "\n",
    "    f.write(test_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
